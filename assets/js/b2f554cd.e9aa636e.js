"use strict";(self.webpackChunkwpyoga_docusaurus_blog=self.webpackChunkwpyoga_docusaurus_blog||[]).push([[1477],{10:function(e){e.exports=JSON.parse('{"blogPosts":[{"id":"/2022/04/27/linux-mint-oem-kernel","metadata":{"permalink":"/blog/2022/04/27/linux-mint-oem-kernel","editUrl":"https://github.com/wpyoga/wpyoga.github.io/edit/master/blog/2022-04-27-linux-mint-oem-kernel.md","source":"@site/blog/2022-04-27-linux-mint-oem-kernel.md","title":"Linux Mint OEM Kernel","description":"The ThinkPad T14 Gen 1 AMD was released in 2020, and it is actually one of the newest / youngest laptops I ever had. Since I am using Linux Mint, which is based on Ubuntu LTS, the kernel is relatively old and I realized that I need to update to a new kernel if I want to have good hardware compatibility.","date":"2022-04-27T00:00:00.000Z","formattedDate":"April 27, 2022","tags":[{"label":"thinkpad","permalink":"/blog/tags/thinkpad"},{"label":"t14","permalink":"/blog/tags/t-14"},{"label":"linux","permalink":"/blog/tags/linux"},{"label":"kernel","permalink":"/blog/tags/kernel"}],"truncated":true,"authors":[],"nextItem":{"title":"ThinkPad T14 TrackPoint & Linux","permalink":"/blog/2022/04/27/thinkpad-t14-trackpoint-linux"}},"content":"The ThinkPad T14 Gen 1 AMD was released in 2020, and it is actually one of the newest / youngest laptops I ever had. Since I am using Linux Mint, which is based on Ubuntu LTS, the kernel is relatively old and I realized that I need to update to a new kernel if I want to have good hardware compatibility.\\n\\nThe latest release of Linux Mint (20.3) is still based on Ubuntu 20.04 LTS, not Ubuntu 22.04 LTS which was just released -- usually Linux Mint will get a new release a few months after the Ubuntu LTS release. Therefore, I\'m upgrading to the latest kernel available in the Ubuntu 20.04 repositories.\\n\\n\x3c!-- truncate --\x3e\\n\\n# Install the OEM kernel\\n\\nThe installation in quite straightforward:\\n\\n```shell-session\\n$ sudo apt install linux-image-oem-20.04 linux-headers-oem-20.04\\n```\\n\\nAfter installation, I rebooted the laptop and chose the corresponding kernel on the boot menu.\\n\\n# Remove the other kernels\\n\\nI then checked that nothing is badly broken, and proceeded to remove the older kernels:\\n\\n```shell-session\\n$ sudo apt purge --autoremove linux-generic-hwe-20.04 \\n$ sudo apt purge --autoremove linux-*-5.4.* linux-*-5.13.*\\n```\\n\\n# Note when upgrading to the next Linux Mint version\\n\\nWhen I eventually upgrade to the next Linux Mint version, I think I would need to install the regular kernel packages again."},{"id":"/2022/04/27/thinkpad-t14-trackpoint-linux","metadata":{"permalink":"/blog/2022/04/27/thinkpad-t14-trackpoint-linux","editUrl":"https://github.com/wpyoga/wpyoga.github.io/edit/master/blog/2022-04-27-thinkpad-t14-trackpoint-linux.md","source":"@site/blog/2022-04-27-thinkpad-t14-trackpoint-linux.md","title":"ThinkPad T14 TrackPoint & Linux","description":"On the ThinkPad T14, the TrackPoint does not work so well in Linux. Pointer movement is choppy and is just not smooth overall. When debugged using libinput debug-events, we can see that the polling rate is only about 30 Hz.","date":"2022-04-27T00:00:00.000Z","formattedDate":"April 27, 2022","tags":[{"label":"thinkpad","permalink":"/blog/tags/thinkpad"},{"label":"t14","permalink":"/blog/tags/t-14"},{"label":"trackpoint","permalink":"/blog/tags/trackpoint"},{"label":"linux","permalink":"/blog/tags/linux"},{"label":"mouse","permalink":"/blog/tags/mouse"}],"truncated":true,"authors":[],"prevItem":{"title":"Linux Mint OEM Kernel","permalink":"/blog/2022/04/27/linux-mint-oem-kernel"},"nextItem":{"title":"ThinkPad T14","permalink":"/blog/2022/04/27/thinkpad-t14"}},"content":"On the ThinkPad T14, the TrackPoint does not work so well in Linux. Pointer movement is choppy and is just not smooth overall. When debugged using `libinput debug-events`, we can see that the polling rate is only about 30 Hz.\\n\\nHowever, there is a workaround.\\n\\n\x3c!-- truncate --\x3e\\n\\n# Workaround\\n\\nThe trick is to reload the `psmouse` module, and [force it](https://forums.lenovo.com/t5/Fedora/T14s-AMD-Trackpoint-almost-unusable/m-p/5064952?page=5#5494798) to use the ImPS/2 protocol.\\n\\n```shell-session\\n$ sudo rmmod psmouse\\n$ sudo modprobe psmouse proto=imps\\n```\\n\\nThis works on Ubuntu-based distros. For Fedora, since the psmouse module is actually built into the kernel, the `proto` parameter has to be passed on boot, appended to the Linux kernel command line:\\n\\n```shell-session\\npsmouse.proto=imps\\n```\\n\\nAfter applying this workaround, the polling rate goes up to about 80 Hz. However, there are two issues:\\n- Pointer movement is now too slow\\n- The TouchPad doesn\'t work at all\\n\\nI don\'t use the TouchPad, so it\'s not an issue for me. The slow pointer movement can be alleviated somewhat.\\n\\n# Speed up pointer movement\\n\\nIn order to speed up the pointer movement, we can use a Coordinate Transformation Matrix:\\n- https://askubuntu.com/a/1308058\\n- https://unix.stackexchange.com/a/177640\\n- https://wiki.ubuntu.com/X/InputCoordinateTransformation\\n\\nYou can play around with the matrix values, but this works best for me:\\n```shell-session\\nxinput set-prop \'PS/2 Synaptics TouchPad\' \'Coordinate Transformation Matrix\' 3 0 0 0 3 0 0 0 1\\n```\\n\\nThen, open the XFCE mouse settings and adjust the Acceleration settings. For me, `3.0` gives me the best results.\\n\\nThe numbers above may vary between ThinkPad models, and even between different TrackPoint brands within the same model.\\n\\n# Notes\\n\\nOn older ThinkPads, setting the `rate`, `speed`, and `sensitivity` via sysfs works. However, it doesn\'t work on the T14.\\n\\n# TODO\\n\\nThe above is just a workaround, and you can even call it a kludge.\\n\\nThe proper fix might be two-fold:\\n- Fix the psmouse driver to recognize and work around the faulty TrackPoint\\n- Implement [constant scale factor](https://gitlab.freedesktop.org/libinput/libinput/-/issues/281) for libinput\\n\\nNote:\\n- This might be related to the fact that the T450s TrackPoint [sometimes stutters](https://www.reddit.com/r/thinkpad/comments/fmt09q/trackpad_sometimes_interferes_with_trackpoint/)"},{"id":"/2022/04/27/thinkpad-t14","metadata":{"permalink":"/blog/2022/04/27/thinkpad-t14","editUrl":"https://github.com/wpyoga/wpyoga.github.io/edit/master/blog/2022-04-27-thinkpad-t14.md","source":"@site/blog/2022-04-27-thinkpad-t14.md","title":"ThinkPad T14","description":"I just got a ThinkPad T14 Gen 1 AMD.","date":"2022-04-27T00:00:00.000Z","formattedDate":"April 27, 2022","tags":[{"label":"thinkpad","permalink":"/blog/tags/thinkpad"},{"label":"t14","permalink":"/blog/tags/t-14"},{"label":"linux","permalink":"/blog/tags/linux"},{"label":"windows","permalink":"/blog/tags/windows"}],"truncated":true,"authors":[],"prevItem":{"title":"ThinkPad T14 TrackPoint & Linux","permalink":"/blog/2022/04/27/thinkpad-t14-trackpoint-linux"},"nextItem":{"title":"Migrating Windows 10 from HDD to SSD using Linux","permalink":"/blog/2022/03/17/migrating-windows-10"}},"content":"I just got a ThinkPad T14 Gen 1 AMD.\\n\\n\x3c!-- truncate --\x3e\\n\\n# Issues\\n\\n- Sleep mode drains the battery\\n    - The T14 has S0ix (\\"Windows\\") and S3 (\\"Linux\\") sleep modes\\n    - S3 sleep drains more power than S0ix\\n- Sleep mode sometimes doesn\'t work\\n    - This seems to be a kernel version problem, and also a distro problem\\n    - Sometimes the laptop does not go to sleep, even if the lid is closed\\n- Waking up from sleep takes ~10 seconds\\n    - Waking up from sleep is instantaneous on Windows \\n- TrackPoint stutters\\n- Screen ghosting / burn-in\\n    - Only visible on a gray screen\\n- Edges of the palmrest are sharp, unlike previous ThinkPads where the palmrest edges are slightly curved\\n\\n# Improvements over previous ThinkPads\\n\\n- Battery charge control now works by firmware\\n    - works without booting into the system\\n    - can be configured under `/sys/class/power_supply/BAT0`:\\n        - `charge_control_end_threshold`\\n        - `charge_control_start_threshold`\\n- Thinner and lighter"},{"id":"/2022/03/17/migrating-windows-10","metadata":{"permalink":"/blog/2022/03/17/migrating-windows-10","editUrl":"https://github.com/wpyoga/wpyoga.github.io/edit/master/blog/2022-03-17-migrating-windows-10.md","source":"@site/blog/2022-03-17-migrating-windows-10.md","title":"Migrating Windows 10 from HDD to SSD using Linux","description":"Nowadays, with the price of SSDs getting cheaper all the time, we would often upgrade an existing laptop HDD into an SSD. And most of the time, in order to do that, it is best to migrate the existing Windows installation on the HDD.","date":"2022-03-17T00:00:00.000Z","formattedDate":"March 17, 2022","tags":[{"label":"win10","permalink":"/blog/tags/win-10"},{"label":"hdd","permalink":"/blog/tags/hdd"},{"label":"ssd","permalink":"/blog/tags/ssd"},{"label":"migration","permalink":"/blog/tags/migration"},{"label":"linux","permalink":"/blog/tags/linux"},{"label":"parted","permalink":"/blog/tags/parted"}],"truncated":true,"authors":[],"prevItem":{"title":"ThinkPad T14","permalink":"/blog/2022/04/27/thinkpad-t14"},"nextItem":{"title":"Show Items in Folder","permalink":"/blog/2022/02/14/show-in-folder"}},"content":"Nowadays, with the price of SSDs getting cheaper all the time, we would often upgrade an existing laptop HDD into an SSD. And most of the time, in order to do that, it is best to migrate the existing Windows installation on the HDD.\\n\\nMigrating a Windows 10 installation from HDD to SSD is usually done using Windows tools like the [Samsung Data Migration Software](https://semiconductor.samsung.com/consumer-storage/support/tools/) or various forms of AOMEI Partition Assistant. I never trusted the latter application 100%, to be very honest.\\n\\nAlso, this time I don\'t have Admin access on the laptop to be migrated. At least not on Windows 10. But the BIOS (UEFI) is unlocked, so we can at least boot a Linux live ISO.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Hardware preparation\\n\\nFirst, we prepare these tools:\\n- screwdrivers\\n- an external HDD case\\n- a bootable Linux live ISO\\n- a bootable Windows 10 installer\\n\\n:::warn\\nIf the original Windows 10 installation is 32-bit, then you have to use a 32-bit Windows 10 installer for the last step.\\n:::\\n\\n:::tip\\nFor the Linux live ISO and the Windows 10 installer, use [Ventoy](https://ventoy.net). Then you can have both on one single USB stick.\\n:::\\n\\nWe remove the existing HDD, and then install the new SSD inside the laptop. This is usually done together with a RAM upgrade, because RAM is relatively cheap nowadays.\\n\\nAfter that, install the old HDD into the external HDD case.\\n\\n## Do the migration\\n\\n1. Boot the laptop into the Linux live ISO.\\n1. Plug the external HDD case into the laptop.\\n1. Use `lsblk` to help identify the HDD and SSD device nodes.\\n1. Use `fdisk -l /dev/sdX` to verify the identities of the HDD and SSD device nodes.\\n    - Let\'s say the HDD is `/dev/sdc` and the SSD is `/dev/sda`\\n1. Copy the partition layout of the HDD and apply it onto the SSD: `sfdisk -d /dev/sdc | sfdisk /dev/sda`\\n1. Open gparted, and copy the original partitions on the HDD onto the SSD. Usually, there will be at least 2 partitions on the original HDD, one for the EFI partition and another one for the Windows partition.\\n1. Still in gparted, resize the Windows partition to fill up all the available space (if needed).\\n1. Reboot into the Windows 10 installer, and then repair the Windows boot manager:\\n    ```\\n    bootrec /RebuildBcd\\n    bootrec /fixMbr\\n    bootrec /fixboot\\n    bootsect /nt60 SYS\\n    ```\\n\\n:::note\\nRead the `sfdisk` man page for more options, and also consider backing up all partition tables before doing anything.\\n:::\\n\\n## What if...?\\n\\n### What if the original Windows partitions don\'t fit on the SSD?\\n\\nResize your Windows partition before replacing the HDD. Make it as small as possible, and don\'t worry because you can always extend it using gparted later.\\n\\nAlternatively, you can try to resize your Windows partition using gparted. Others have had [success](https://superuser.com/questions/821131/is-it-safe-to-resize-windows-partition-with-gparted), but I haven\'t tried it yet.\\n\\n## What if I don\'t have a Linux live ISO with a desktop?\\n\\nYou can still copy the partitions using `dd`. Then you can use `parted` or `sfdisk` or even `fdisk` to resize the partition, and `ntfsresize` to grow the filesystem."},{"id":"/2022/02/14/show-in-folder","metadata":{"permalink":"/blog/2022/02/14/show-in-folder","editUrl":"https://github.com/wpyoga/wpyoga.github.io/edit/master/blog/2022-02-14-show-in-folder.md","source":"@site/blog/2022-02-14-show-in-folder.md","title":"Show Items in Folder","description":"When we download a file in the browser, there is usually an option to \\"Show in folder\\". For the longest time, I\'ve been looking for a way to do replicate this functionality on the command line.","date":"2022-02-14T00:00:00.000Z","formattedDate":"February 14, 2022","tags":[{"label":"gui","permalink":"/blog/tags/gui"},{"label":"file manager","permalink":"/blog/tags/file-manager"},{"label":"dbus","permalink":"/blog/tags/dbus"}],"truncated":true,"authors":[],"prevItem":{"title":"Migrating Windows 10 from HDD to SSD using Linux","permalink":"/blog/2022/03/17/migrating-windows-10"},"nextItem":{"title":"Libvirt Networking Configuration","permalink":"/blog/2022/01/27/libvirt-networking"}},"content":"When we download a file in the browser, there is usually an option to \\"Show in folder\\". For the longest time, I\'ve been looking for a way to do replicate this functionality on the command line.\\n\\n\x3c!-- truncate --\x3e\\n\\n## It\'s not xdg-open\\n\\nWe know that we can use `xdg-open` to open a file from the command line. By \\"open\\" I mean call the associated application and have it open the file requested. So I tried to monitor invocations of `xdg-open` by using the stupidest method there is:\\n\\n```title=\\"xdg-open\\"\\n#!/bin/sh\\necho \\"$0\\" \\"$@\\" >>/tmp/xdg-open-invocations.log\\nxdg-open-real \\"$@\\"\\n```\\n\\nOf course, I\'ve renamed the original `xdg-open` binary to `xdg-open-real`.\\n\\nBut I found nothing, no logs.\\n\\n## It\'s related to the file manager\\n\\nI applied the \\"monitoring\\" process to Thunar, but I only see a line like this in the log:\\n\\n```\\n/usr/bin/Thunar --daemon\\n```\\n\\nSomething\'s calling `Thunar` and making it run as a daemon (presumably for faster start-up performance for subsequent calls). But I don\'t see anything related to the file I was viewing. Apparently `Thunar` doesn\'t have an option to do that, either.\\n\\n## Monitoring all process invocations\\n\\nSo I googled a bit and found [this](https://unix.stackexchange.com/questions/260162/how-to-track-newly-created-processes-in-linux). The proper solution is:\\n\\n```\\n$ sudo bpftrace -e \'tracepoint:syscalls:sys_enter_exec*{ printf(\\"pid: %d, comm: %s, args: \\", pid, comm); join(args->argv); }\'\\n```\\n\\n:::warning\\nDO NOT copy solutions blindly from StackOverflow / Google / etc. Understand what it does first, then adapt it or use it accordingly.\\n:::\\n\\nHowever, it doesn\'t show any process invocations other than the `Thunar --daemon` one.\\n\\n## Of course it had to be D-Bus\\n\\nThen it dawned on me that this should concern D-Bus. Using `dbus-monitor`, we can see this:\\n\\n```\\nmethod call time=1644810719.430850 sender=:1.640 -> destination=org.freedesktop.FileManager1 serial=12 path=/org/freedesktop/FileManager1; interface=org.freedesktop.FileManager1; member=ShowItems\\n   array [\\n      string \\"file:///home/william/Downloads/download.pdf\\"\\n   ]\\n   string \\"\\"\\n```\\n\\nSo I can replay it:\\n\\n```\\n$ dbus-send --type=method_call --dest=org.freedesktop.FileManager1 /org/freedesktop/FileManager1 org.freedesktop.FileManager1.ShowItems array:string:\\"file:///home/william/Downloads/download.pdf\\" string:\\"\\"\\n```\\n\\nAnd it works! So I created this script to help me:\\n\\n```title=\\"bin/show-in-folder\\"\\n#!/bin/sh\\n\\nPATH_ARRAY=\\nfor x in \\"$@\\"; do\\n  PATH_ARRAY=\\"$PATH_ARRAY\\",\\"file://$(pwd)/$x\\"\\ndone\\n\\ndbus-send \\\\\\n  --type=method_call \\\\\\n  --dest=org.freedesktop.FileManager1 \\\\\\n  /org/freedesktop/FileManager1 \\\\\\n  org.freedesktop.FileManager1.ShowItems \\\\\\n  array:string:\\"${PATH_ARRAY#,}\\" \\\\\\n  string:\\"\\"\\n```"},{"id":"/2022/01/27/libvirt-networking","metadata":{"permalink":"/blog/2022/01/27/libvirt-networking","editUrl":"https://github.com/wpyoga/wpyoga.github.io/edit/master/blog/2022-01-27-libvirt-networking.md","source":"@site/blog/2022-01-27-libvirt-networking.md","title":"Libvirt Networking Configuration","description":"If you use a wireless adapter on your VM host, libvirt networking is complicated. We have 3 options:","date":"2022-01-27T00:00:00.000Z","formattedDate":"January 27, 2022","tags":[{"label":"libvirt","permalink":"/blog/tags/libvirt"},{"label":"networking","permalink":"/blog/tags/networking"},{"label":"bridged","permalink":"/blog/tags/bridged"},{"label":"routed","permalink":"/blog/tags/routed"},{"label":"nat","permalink":"/blog/tags/nat"},{"label":"dmz","permalink":"/blog/tags/dmz"}],"truncated":true,"authors":[],"prevItem":{"title":"Show Items in Folder","permalink":"/blog/2022/02/14/show-in-folder"},"nextItem":{"title":"Installing Proxmox VE on Debian Bullseye","permalink":"/blog/2022/01/26/installing-proxmox-ve-on-debian-bullseye"}},"content":"If you use a wireless adapter on your VM host, libvirt networking is complicated. We have 3 options:\\n\\n- Bridged (with parprouted)\\n- Routed\\n- NAT (the default)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Bridged\\n\\nIn this scenario, the virtual machines appear as \\"real\\" nodes in the local network. Using wired connections (Ethernet) it\'s very easy to set up. However, due to [the way wireless networks work](https://superuser.com/a/1009881), bridging becomes a bit more complicated.\\n\\nThe core issue is: when a wireless client wants to send a packet to another host, it has to send 3 MAC addresses to the AP:\\n- AP\'s MAC address\\n- Client\'s MAC address\\n- Destination MAC address\\n\\nRemember that in AP / Managed mode, all communication has to pass through the AP. And each client to the AP needs to fill in its own MAC address as the Source address. If the client sends a MAC address belonging to a hosted virtual machine, that MAC address does not match the authenticated client\'s MAC address, so the AP will reject it.\\n\\nTo solve this issue, we need to use an arp proxy. The manual way of doing this is using the `arp` utility. The automatic way is using `parprouted`. With an ARP proxy in place, the wireless client will send out forwarded packets with the MAC address changed to its authenticated MAC address. When packets come back, the MAC address is still the client\'s MAC address, but the IP address is the VM\'s IP address. The ARP proxy sees the destination IP address, looks it up in a table, and replaces the destination MAC address of the packet to be the destination VM\'s MAC address.\\n\\n## Routed\\n\\nRouted is basically NAT, with 2 major differences:\\n- Incoming connections are allowed and routed to the requested hosts\\n- Outgoing connections still bear their original IP address\\n\\nHowever, without a static route set up on the gateway, the VM\'s cannot access anything outside the local network. When it tries to do so, the gateway will be unable to return packets -- it doesn\'t know where to send the packets to.\\n\\nSetting up static routes on other machines in the local network only enable communication between the nodes and the VM\'s.\\n\\n## NAT\\n\\nWith this set up:\\n- Incoming connections are not allowed, unless initiated by a VM. This means nodes in the local network cannot access VM\'s.\\n- Outgoing connections are masqueraded, so the gateway knows where to send the return packets to. This means the VM\'s can access the local network and everything outside the local network.\\n\\nTo enable seamless internode communication, we need to:\\n- Allow incoming connections. This effectively makes the whole NAT subnet be a DMZ.\\n- Forward connections to nested VM\'s within virtualized NAT networks.\\n\\nTo allow incoming connections, we need to do two things:\\n- remove this rule: `-A LIBVIRT_FWI -o virbr0 -j REJECT --reject-with icmp-port-unreachable`\\n- forward connections:\\n    - 192.168.124.0/24 via 192.168.122.223\\n    - 192.168.125.0/24 via 192.168.122.74\\n    - 192.168.126.0/24 via 192.168.122.91\\n\\n```xml\\n<route address=\\"192.168.124.0\\" prefix=\\"24\\" gateway=\\"192.168.122.223\\"/>\\n<route address=\\"192.168.125.0\\" prefix=\\"24\\" gateway=\\"192.168.122.74\\"/>\\n<route address=\\"192.168.126.0\\" prefix=\\"24\\" gateway=\\"192.168.122.91\\"/>\\n```\\n\\nTo enable forwarding, we need to manipulate iptables rules using hooks. Be careful though, when a qemu hook is called, and a non-zero exit status is returned, all domains are destroyed.\\n\\nIf a hook script is not executable, then a log line will be printed on libvirtd\'s log. But domains can be started and stopped.\\n\\n## Open\\n\\nWith \\"Open\\" mode, no iptables rules are set by default. We would then need to specify our own rules.\\n\\nTODO: Propose a \\"Custom\\" mode, so that we don\'t need to use hooks\\n\\n\\n### Events\\n\\nTODO: propose a multi-level tree of drop-in scripts:\\nmake a sample implementation using scripts\\n\\n/etc/libvirtd/hooks/qemu.d/guest1/prepare/begin\\n/etc/libvirtd/hooks/qemu.d/guest1/start/begin\\n/etc/libvirtd/hooks/qemu.d/guest1/started/begin\\n/etc/libvirtd/hooks/qemu.d/guest1/stopped/end\\n\\n\\n\\nhttps://libvirt.org/hooks.html\\n\\n- virsh net-destroy\\niptables rules for the network are deleted\\n```\\n/etc/libvirt/hooks/network default stopped end -\\n```\\n\\n- virsh net-start\\niptables rules for the network are applied\\n```\\n/etc/libvirt/hooks/network default start begin -\\n/etc/libvirt/hooks/network default started begin -\\n```\\n\\n- virsh start\\n```\\n/etc/libvirt/hooks/qemu ub16 prepare begin -\\n/etc/libvirt/hooks/network default port-created begin -\\n/etc/libvirt/hooks/qemu ub16 start begin -\\n/etc/libvirt/hooks/qemu ub16 started begin -\\n```\\nif a domain is started but its network is not active, the domain will fail to start\\n```\\n/etc/libvirt/hooks/qemu ub16 prepare begin -\\n/etc/libvirt/hooks/qemu ub16 stopped end -\\n/etc/libvirt/hooks/qemu ub16 release end -\\n```\\n\\n- virsh shutdown / virsh destroy\\n```\\n/etc/libvirt/hooks/qemu ub16 stopped end -\\n/etc/libvirt/hooks/network default port-deleted begin -\\n/etc/libvirt/hooks/qemu ub16 release end -\\n```\\n\\n- systemctl stop libvirtd\\niptables rules are not touched\\n```\\n/etc/libvirt/hooks/daemon - shutdown - shutdown\\n```\\n\\n- systemctl start libvirtd\\niptables rules for the active networks are applied\\n```\\n/etc/libvirt/hooks/daemon - start - start\\n```\\nnetworks marked as \\"Autostart\\" are not started\\ndomains that are running trigger these events\\n```\\n/etc/libvirt/hooks/network default port-created begin -\\n/etc/libvirt/hooks/qemu ovz6 reconnect begin -\\n```\\n\\n- systemctl restart libvirtd\\nsame as stop then immediately start\\n\\n- systemctl reload libvirtd\\nthe daemon hook is run before the iptables rules are applied\\niptables rules for the active networks are applied\\n```\\n/etc/libvirt/hooks/daemon - reload begin SIGHUP\\n```\\nif some networks marked as \\"Autostart\\" are not active, then those networks are started, and triggers these events:\\n```\\n/etc/libvirt/hooks/network default start begin -\\n/etc/libvirt/hooks/network default started begin -\\n```\\nif libvirtd is running, base libvirt chains are created:\\n```\\n*mangle\\n-A POSTROUTING -j LIBVIRT_PRT\\n*nat\\n-A POSTROUTING -j LIBVIRT_PRT\\n*filter\\n-A INPUT -j LIBVIRT_INP\\n-A FORWARD -j LIBVIRT_FWX\\n-A FORWARD -j LIBVIRT_FWI\\n-A FORWARD -j LIBVIRT_FWO\\n-A OUTPUT -j LIBVIRT_OUT\\n```"},{"id":"/2022/01/26/installing-proxmox-ve-on-debian-bullseye","metadata":{"permalink":"/blog/2022/01/26/installing-proxmox-ve-on-debian-bullseye","editUrl":"https://github.com/wpyoga/wpyoga.github.io/edit/master/blog/2022-01-26-installing-proxmox-ve-on-debian-bullseye.md","source":"@site/blog/2022-01-26-installing-proxmox-ve-on-debian-bullseye.md","title":"Installing Proxmox VE on Debian Bullseye","description":"I tried to install Proxmox VE 7.1 as a VM using libvirt, but ran into two issues:","date":"2022-01-26T00:00:00.000Z","formattedDate":"January 26, 2022","tags":[{"label":"proxmox","permalink":"/blog/tags/proxmox"},{"label":"debian","permalink":"/blog/tags/debian"}],"truncated":true,"authors":[],"prevItem":{"title":"Libvirt Networking Configuration","permalink":"/blog/2022/01/27/libvirt-networking"},"nextItem":{"title":"Vaccination Certificate Download Via PeduliLindungi","permalink":"/blog/2022/01/24/vaccine-certrificate"}},"content":"I tried to install Proxmox VE 7.1 as a VM using libvirt, but ran into two issues:\\n\\n- The official ISO image cannot be booted as a regular Linux installation ISO, because it does not have Joliet extensions\\n- The official installer is GUI-based, so we cannot install it over a serial console\\n\\n\x3c!-- truncate --\x3e\\n\\n## Background\\n\\nI wanted to learn Proxmox. Since I already have a VM server using libvirt, I figured I should install Proxmox VE as a virtual machine. I\'ve installed OpenVZ 6 and 7 before as virtual machines, so it should be straightforward.\\n\\nHowever, as mentioned above, I ran into two issues. Therefore I will try to install Proxmox on Debian Bullseye instead, following this tutorial: https://pve.proxmox.com/wiki/Install_Proxmox_VE_on_Debian_11_Bullseye\\n\\n## Planning\\n\\nBase OS: Debian 11.2.0\\nNetwork: 192.168.126.0/24\\n\\n## Base OS installation\\n\\nFirst, install Debian 11 as usual. The thing about Debian installers is that the location we choose determines our timezone. Suppose I live in Asia, and I would like my laptop to have the English (US) locale, what should I do?\\n\\nTherefore I usually pretend I\'m in the USA, then change the timezone after installation is complete. This is much easier than changing the locale.\\n\\n## Proxmox-specific changes\\n\\nAfter installation, `/etc/hosts` will look something like this:\\n\\n```\\n127.0.0.1\\tlocalhost\\n127.0.1.1\\tmy-hostname\\n\\n# The following lines are desirable for IPv6 capable hosts\\n::1     localhost ip6-localhost ip6-loopback\\nff02::1 ip6-allnodes\\nff02::2 ip6-allrouters\\n```\\n\\nFirst, check the ip address of the node. An internal IP address is fine. This can be done using `ip addr`.\\n\\nRemove the line `127.0.1.1\\tmy-hostname` and replace it with the reachable IP address:\\n\\n```\\n192.168.122.91\\tmy-hostname\\n```\\n\\nNote to self: submit a correction or register an account: https://forum.proxmox.com/threads/how-can-we-contribute-to-the-wiki.93970/\\n\\n## Reboot\\n\\nNow we shut down the VM, and enable nested virtualization for the libvirt domain: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_virtualization/creating-nested-virtual-machines_configuring-and-managing-virtualization\\n\\nThen start the domain again, which will boot into the Proxmox kernel.\\n\\n## Create a non-root user\\n\\n```shell-session\\n$ sudo pveum useradd william@pve\\n$ sudo pveum passwd william@pve\\n```\\n\\nNote that `pve` is the realm. This is not the node\'s hostname.\\n\\nAlso create a group for the user:\\n\\n```shell-session\\n$ sudo pveum group add admin\\n$ sudo pveum user modify william@pve -append -groups admin\\n```\\n\\nThen grant permissions to the group:\\n\\n```shell-session\\n$ sudo pveum acl modify / --roles Administrator --groups admin\\n```\\n\\n## Web GUI\\n\\nOpen https://192.168.122.91:8006/ in a browser. Note that this is only accessible via HTTPS. Plain HTTP won\'t work.\\n\\nNote to self: try to configure a redirect here (HTTP -> HTTPS)\\n\\n## Run Proxmox without a subscription\\n\\nRemove `/etc/apt/sources.list.d/pve-enterprise.list`\\n\\nNote to self: try to figure out how to remove this automatically.\\n\\n## Create a network bridge\\n\\nSince my home server is connected to the router via Wi-Fi, let\'s create a bridge for a routed network.\\nhttps://192.168.122.91:8006/pve-docs/chapter-sysadmin.html#_routed_configuration\\n\\nThe bridge should get an IP address of .1 in a new subnet, and there should be no interface added to it.\\nIn my example, the CIDR is 192.168.126.1/24.\\n\\n## Physical host network\\n\\non the physical host\\n\\n```shell-session\\n$ sudo ip route add 192.168.126.0/24 via 192.168.122.91\\n```\\n\\nor:\\n\\n```shell-session\\n$ virsh net-edit default && virsh net-destroy default && virsh net-start default && sudo systemctl restart libvirtd\\n```\\n\\n```xml\\n<network>\\n  ...\\n  <route family=\'ipv4\' address=\'192.168.126.0\' prefix=\'24\' gateway=\'192.168.122.91\'/>\\n</network>\\n```\\n\\n```\\n$ sudo iptables -I FORWARD -s 192.168.126.0/24 -i virbr0 -j ACCEPT\\n$ sudo iptables -I FORWARD -d 192.168.126.0/24 -o virbr0 -j ACCEPT\\n$ sudo iptables -t nat -A POSTROUTING -s 192.168.126.0/24 ! -d 192.168.126.0/24 -j MASQUERADE\\n```\\n\\ninstall iptables-persistent\\nthen create /etc/iptables/rules.v4\\n```\\n*filter\\n-A FORWARD -d 192.168.126.0/24 -o virbr0 -j ACCEPT\\n-A FORWARD -s 192.168.126.0/24 -i virbr0 -j ACCEPT\\n\\n*nat\\n-A POSTROUTING -s 192.168.126.0/24 ! -d 192.168.126.0/24 -j MASQUERADE\\n```\\n\\n\\n## Install dnsmasq to provide DHCP server\\n\\nFrom: https://bobcares.com/blog/dnsmasq-dhcp-server-in-proxmox/\\n\\n```\\n$ sudo apt install dnsmasq\\n```\\n\\nConfigure it:\\n```title=/etc/dnsmasq.conf\\ndomain-needed\\nbogus-priv\\ninterface=vmbr0\\ndhcp-range=192.168.126.2,192.168.126.254,12h\\n```\\n\\nThen restart the dnsmasq service.\\n\\n## Use systemd-resolved to provide DHCP server\\n\\nhttps://www.freedesktop.org/software/systemd/man/systemd.network.html\\n\\nCreate a file `/etc/systemd/network/proxmox.network`:\\n```ini\\n[Match]\\nName=vmbr*\\nDriver=bridge\\n\\n[Network]\\nAddress=192.168.126.1/24\\nDHCPServer=yes\\nIPMasquerade=yes\\n\\n[DHCPServer]\\nDNS=8.8.8.8\\n```\\n\\nThis will enable IP forwarding etc.\\n\\n## Creating a VM\\n\\nPut ISO files in `/var/lib/vz/template/iso`"},{"id":"/2022/01/24/vaccine-certrificate","metadata":{"permalink":"/blog/2022/01/24/vaccine-certrificate","editUrl":"https://github.com/wpyoga/wpyoga.github.io/edit/master/blog/2022-01-24-vaccine-certrificate.md","source":"@site/blog/2022-01-24-vaccine-certrificate.md","title":"Vaccination Certificate Download Via PeduliLindungi","description":"Today I needed my Vaccination certificates in order to apply for my third shot. It was not as straightforward as it seemed.","date":"2022-01-24T00:00:00.000Z","formattedDate":"January 24, 2022","tags":[{"label":"covid19","permalink":"/blog/tags/covid-19"},{"label":"vaccine","permalink":"/blog/tags/vaccine"}],"truncated":true,"authors":[],"prevItem":{"title":"Installing Proxmox VE on Debian Bullseye","permalink":"/blog/2022/01/26/installing-proxmox-ve-on-debian-bullseye"},"nextItem":{"title":"When My Server\'s Wi-Fi Got Disconnected","permalink":"/blog/2022/01/22/server-wifi-disconnected"}},"content":"Today I needed my Vaccination certificates in order to apply for my third shot. It was not as straightforward as it seemed.\\n\\nIf you live in Indonesia, you probably have a PeduliLindungi account, and have the app installed on your phone. It works for downloading the certificates, but I had to rummage through my phone storage in order to access it.\\n\\n\x3c!-- truncate --\x3e\\n\\n## First attempt using PeduliLindungi\\n\\nI opened my PeduliLindungi app, and proceeded to download both vaccine certificates. The downloads immediately succeeded, and I was left wondering: how do I access those downloaded files, and where are they stored on my phone?\\n\\nI did not think to open my Gallery app, and it\'s not something I like to do, just because it\'s very slow. Also, the vaccine certificate may very well have been in PDF format, right? I mean, it\'s not like the government doesn\'t know that sending pictures over WhatsApp (a very common use case) degrades its quality, right? They are *competent*, right?\\n\\nSo anyway, I googled for the download location, but instead found links explaining how to download the vaccine certificates without the PeduliLindungi app. Wow, this is great, or so I thought.\\n\\n## Second attempt using WhatsApp\\n\\nFollowing the instructions, I messaged [+6281110500567](https://wa.me/6281110500567) and was greeted with a menu, as described.\\n\\nSo I chose to download my vaccination certificates using the menu, as instructed.\\n\\nAfter completing the OTP and ID verification process, it gave me two options: download vaccine certificate 1 or 2.\\n\\nSo I selected `1` and it says:\\n\\n> Berikut sertifikat dari *NAME_REDACTED*\\n> \\n> Ada lagi yang bisa dibantu?\\n\\nWhat basically happened was that I completed the whole process, and they promised to send me my vaccine certificate, but I didn\'t actually get any.\\n\\nI tried downloading the second vaccine certificate, same results. Not sure if the government is playing a prank on us, or secretly trying to diagnose for insanity among its population.\\n\\n:::tip\\n\\"The definition of insanity is doing the same thing over and over again and expecting different results.\\"\\n:::\\n\\n## Finding the vaccine certificate on my phone\\n\\nSo I opened my trusty Total Commander once again and began looking for the downloaded vaccine certificates, one by one. Finally found it here: `sdcard:Pictures/Vaccine-Certificate`\\n\\nGreat, the vaccine certificates are right there, in JPEG form. I\'m trying to be positive and think that our government is reducing our mobile data burden by serving us JPEG files instead of PNG. Think about all those kids with almost no money hooked on playing mobile games on their phones, with no mobile data quota to spare to download a vaccine certificate. I must applaud the government for setting its priorities straight.\\n\\n## But wait, there\'s more!\\n\\nIt turns out I also needed to download my third vaccine ticket. It\'s conveniently missing from the PeduliLindungi app\'s landing page, and I have to instead open my Account settings, then click on `Vaccination History and Tickets` to get it.\\n\\nThese folks sure know how to test our patience and intellect!"},{"id":"/2022/01/22/server-wifi-disconnected","metadata":{"permalink":"/blog/2022/01/22/server-wifi-disconnected","editUrl":"https://github.com/wpyoga/wpyoga.github.io/edit/master/blog/2022-01-22-server-wifi-disconnected.md","source":"@site/blog/2022-01-22-server-wifi-disconnected.md","title":"When My Server\'s Wi-Fi Got Disconnected","description":"A few days ago, I just installed an NVMe SSD on my home server. Then I went on a quick trip, and found out that I couldn\'t access my server anymore. I had thought to myself, could it be that the SSD was bad? (I bought it used, but barely)","date":"2022-01-22T00:00:00.000Z","formattedDate":"January 22, 2022","tags":[{"label":"wifi","permalink":"/blog/tags/wifi"},{"label":"NetworkManager","permalink":"/blog/tags/network-manager"}],"truncated":true,"authors":[],"prevItem":{"title":"Vaccination Certificate Download Via PeduliLindungi","permalink":"/blog/2022/01/24/vaccine-certrificate"},"nextItem":{"title":"Deciphering Aptitude Search Results","permalink":"/blog/2022/01/19/deciphering-aptitude-search-results"}},"content":"A few days ago, I just installed an NVMe SSD on my home server. Then I went on a quick trip, and found out that I couldn\'t access my server anymore. I had thought to myself, could it be that the SSD was bad? (I bought it used, but barely)\\n\\nIt turns out that the default NetworkManager settings are not suitable for my use-case, as described by [this forum post](https://community.home-assistant.io/t/wifi-does-not-reconnect-if-router-was-gone/247532/5):\\n\\n> The [nmcli docs](https://developer.gnome.org/NetworkManager/stable/settings-connection.html) state:\\n> \\n> > The number of times a connection should be tried when autoactivating before giving up. Zero means forever, -1 means the global default (4 times if not overridden).\\n> \\n> Four? Why four? Why not 42? What kind of default is that supposed to be?\\n> Kind of \u201cPlease try, but not that hard.\u201d \ud83d\ude42\\n\\nBefore you say anything, yes I use a Wi-Fi adapter for my home server. It\'s just an educational server running a few VM\'s, so Wi-Fi makes sense here. Running a cable between the server and the router would have complicated the installation.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Initial discovery\\n\\nI was in my hotel room, checking up on my home server, when I found that it cannot be accessed. Note that I live in a third-world country, where IP addresses are not given out to home broadband subscribers anymore, so I have to rely on a reverse proxy or VPN to access my home network.\\n\\nI checked my reverse proxy, the proxy server is up and running. When I checked my VPN, it shows that my home server had disconnected the previous night. I had indeed installed an NVMe SSD the previous night, and I thought the SSD might have been corrupted (I\'m running a few VM\'s on it), or maybe the PCIe adapter was broken (no more space for an additional NVMe SSD on the motherboard).\\n\\nAnother possibility that came to mind was, one or more VM\'s used up all the memory, and the VPN client died due to an OOM condition. Not sure if this is possible in reality. Or, maybe the Wi-Fi adapter (it\'s a cheap USB adapter)\\n\\nAnyway, I didn\'t think much about it, and waited until I could get home to diagnose the issue.\\n\\n## Back home\\n\\nAs soon as I got back home, I checked the server and saw that it\'s still powered on. I tried pinging the server and got no response. I tried unplugging and plugging the Wi-Fi adapter, and the server was back online after the second try. It was then that I remembered, I had unplugged the Wi-Fi access point right around the time the VPN client was disconnected.\\n\\n## Figuring out the issue\\n\\nI was hoping for some Wi-Fi driver crash to explain the issue, but found nothing in dmesg. Then I looked at NetworkManager logs and found this:\\n\\n```\\nJan 20 21:46:48 wmpc NetworkManager[734]: <info>  [1642690008.5546] manager: NetworkManager state is now CONNECTED_SITE\\nJan 20 21:46:50 wmpc NetworkManager[734]: <info>  [1642690010.0196] manager: NetworkManager state is now CONNECTED_GLOBAL\\nJan 20 21:57:00 wmpc NetworkManager[734]: <warn>  [1642690620.4089] sup-iface[0x55b52f610120,wlxf428531ddf06]: connection disconnected (reason -4)\\nJan 20 21:57:00 wmpc NetworkManager[734]: <info>  [1642690620.4278] device (wlxf428531ddf06): supplicant interface state: completed -> disconnected\\nJan 20 21:57:00 wmpc NetworkManager[734]: <info>  [1642690620.5123] device (wlxf428531ddf06): supplicant interface state: disconnected -> scanning\\nJan 20 21:57:15 wmpc NetworkManager[734]: <warn>  [1642690635.5544] device (wlxf428531ddf06): link timed out.\\nJan 20 21:57:15 wmpc NetworkManager[734]: <info>  [1642690635.5555] device (wlxf428531ddf06): state change: activated -> failed (reason \'ssid-not-found\', sys-iface-state: \'managed\')\\nJan 20 21:57:15 wmpc NetworkManager[734]: <warn>  [1642690635.5590] device (wlxf428531ddf06): Activation: failed for connection \'mywifi\'\\nJan 20 21:57:15 wmpc NetworkManager[734]: <info>  [1642690635.5604] device (wlxf428531ddf06): state change: failed -> disconnected (reason \'none\', sys-iface-state: \'managed\')\\nJan 20 21:57:15 wmpc NetworkManager[734]: <info>  [1642690635.5837] manager: NetworkManager state is now CONNECTED_LOCAL\\nJan 20 21:57:20 wmpc NetworkManager[734]: <info>  [1642690640.1888] device (wlxf428531ddf06): supplicant interface state: scanning -> inactive\\n```\\n\\nThe link timed out at around 10 PM on Thursday, and it didn\'t reconnect.\\n\\nAfter some googling, I found out the reason here: https://community.home-assistant.io/t/wifi-does-not-reconnect-if-router-was-gone/247532/5\\n\\nWhen I ran `nmcli con show my-network`, I saw these lines in the output:\\n\\n```\\nconnection.autoconnect:                 yes\\nconnection.autoconnect-priority:        0\\nconnection.autoconnect-retries:         -1 (default)\\n```\\n\\n## The solution\\n\\nSo I ran:\\n\\n```shell-session\\n$ sudo nmcli con mod my-network connection.autoconnect-retries 0\\n```\\n\\nThat command updated the configuration into:\\n\\n```\\nconnection.autoconnect:                 yes\\nconnection.autoconnect-priority:        0\\nconnection.autoconnect-retries:         0 (forever)\\n```\\n\\nTo confirm that the settings will also be applied after a reboot, I checked `/etc/NetworkManager/system-connections/my-network.nmconnection`:\\n\\n```ini\\n[connection]\\nid=mywifi\\nuuid=01234567-89ab-cdef-0123-456789abcdef\\ntype=wifi\\nautoconnect-retries=0\\n```\\n\\nWith this setting in place, the configuration will survive reboots."},{"id":"/2022/01/19/deciphering-aptitude-search-results","metadata":{"permalink":"/blog/2022/01/19/deciphering-aptitude-search-results","editUrl":"https://github.com/wpyoga/wpyoga.github.io/edit/master/blog/2022-01-19-deciphering-aptitude-search-results.md","source":"@site/blog/2022-01-19-deciphering-aptitude-search-results.md","title":"Deciphering Aptitude Search Results","description":"The output of apt search xxx is a little cryptic, and I needed to Google quite a bit before landing on the correct documentation//www.debian.org/doc/manuals/aptitude/ch02s02s02.en.html","date":"2022-01-19T00:00:00.000Z","formattedDate":"January 19, 2022","tags":[{"label":"apt","permalink":"/blog/tags/apt"},{"label":"aptitude","permalink":"/blog/tags/aptitude"},{"label":"search","permalink":"/blog/tags/search"},{"label":"debian","permalink":"/blog/tags/debian"}],"truncated":true,"authors":[],"prevItem":{"title":"When My Server\'s Wi-Fi Got Disconnected","permalink":"/blog/2022/01/22/server-wifi-disconnected"},"nextItem":{"title":"Installing OpenVZ 6","permalink":"/blog/2022/01/17/installing-openvz-6"}},"content":"The output of `apt search xxx` is a little cryptic, and I needed to Google quite a bit before landing on the correct documentation: https://www.debian.org/doc/manuals/aptitude/ch02s02s02.en.html\\n\\n\x3c!-- truncate --\x3e\\n\\n## Background\\n\\nToday, as I was updating my laptop, I got these warnings from `apt update`:\\n\\n```\\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: http://repo.mysql.com/apt/ubuntu focal InRelease: The following signatures couldn\'t be verified because the public key is not available: NO_PUBKEY 467B942D3A79BD29\\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: http://deb.anydesk.com all InRelease: The following signatures were invalid: EXPKEYSIG 18DF3741CDFFDE29 philandro Software GmbH <info@philandro.com>\\nW: Failed to fetch http://deb.anydesk.com/dists/all/InRelease  The following signatures were invalid: EXPKEYSIG 18DF3741CDFFDE29 philandro Software GmbH <info@philandro.com>\\nW: Failed to fetch http://repo.mysql.com/apt/ubuntu/dists/focal/InRelease  The following signatures couldn\'t be verified because the public key is not available: NO_PUBKEY 467B942D3A79BD29\\nW: Some index files failed to download. They have been ignored, or old ones used instead.\\n```\\n\\nSo I thought to myself, I haven\'t been using Anydesk and MySQL lately, let\'s remove those packages anyway.\\n\\nFrom [this SE post](https://askubuntu.com/questions/342434/find-what-packages-are-installed-from-a-repository), I learned a few commands:\\n\\n```shell-session\\n$ apt policy | grep mysql\\n 500 http://repo.mysql.com/apt/ubuntu focal/mysql-tools i386 Packages\\n     release o=MySQL,n=focal,l=MySQL,c=mysql-tools,b=i386\\n     origin repo.mysql.com\\n 500 http://repo.mysql.com/apt/ubuntu focal/mysql-tools amd64 Packages\\n     release o=MySQL,n=focal,l=MySQL,c=mysql-tools,b=amd64\\n     origin repo.mysql.com\\n 500 http://repo.mysql.com/apt/ubuntu focal/mysql-8.0 amd64 Packages\\n     release o=MySQL,n=focal,l=MySQL,c=mysql-8.0,b=amd64\\n     origin repo.mysql.com\\n 500 http://repo.mysql.com/apt/ubuntu focal/mysql-apt-config i386 Packages\\n     release o=MySQL,n=focal,l=MySQL,c=mysql-apt-config,b=i386\\n     origin repo.mysql.com\\n 500 http://repo.mysql.com/apt/ubuntu focal/mysql-apt-config amd64 Packages\\n     release o=MySQL,n=focal,l=MySQL,c=mysql-apt-config,b=amd64\\n     origin repo.mysql.com\\n```\\n\\nWith `o=MySQL`, I can see that I need to do:\\n\\n```shell-session\\n$ apt search \\"?origin (MySQL) ?installed\\"\\ni   libmysqlclient21                        - MySQL shared client libraries\\ni   mysql-apt-config                        - Auto configuration for MySQL APT Repo.\\ni A mysql-client                            - MySQL Client meta package depending on latest vers\\ni   mysql-common                            - Common files shared between packages\\ni A mysql-community-client                  - MySQL Client\\ni A mysql-community-client-core             - MySQL Client Core Binaries\\ni A mysql-community-client-plugins          - MySQL Client plugin\\ni   mysql-community-server                  - MySQL Server\\ni A mysql-community-server-core             - MySQL Server Core Binaires\\ni   mysql-workbench-community               - MySQL Workbench\\n```\\n\\nBut what do `i` and `A` mean?\\n\\n## The documentation\\n\\nIt\'s not easy to find this piece of information. I had to Google a whlie to finally find this: https://www.debian.org/doc/manuals/aptitude/rn01re01.en.html\\n\\nUnder `search`, it explains thus:\\n\\n> Each search result is listed on a separate line. The first character of each line indicates the current state of the package: the most common states are p, meaning that no trace of the package exists on the system, c, meaning that the package was deleted but its configuration files remain on the system, i, meaning that the package is installed, and v, meaning that the package is virtual. The second character indicates the stored action (if any; otherwise a blank space is displayed) to be performed on the package, with the most common actions being i, meaning that the package will be installed, d, meaning that the package will be deleted, and p, meaning that the package and its configuration files will be removed. If the third character is A, the package was automatically installed.\\n\\nA more detailed documentation can be found here: https://www.debian.org/doc/manuals/aptitude/ch02s02s02.en.html\\n\\nIn my case, what I need to know is:\\n- `i` means the package is installed\\n- `A` means the package was installed automatically -- this means it\'s a dependency of another package\\n\\nAlright, now that we know, and because we don\'t need those packages,we can remove all the installed packages directly, except the ones marked as \\"automatically installed\\". Those packages should be automatically removed anyway:\\n\\n```shell-session\\n$ sudo apt purge --autoremove mysql-apt-config mysql-common mysql-community-server mysql-workbench-community\\n```\\n\\nAfter that, `apt search \\"?origin (MySQL) ?installed\\"` doesn\'t return anything, so the repo should be safe to remove, or so I thought.\\n\\nIt turns out that removing `mysql-apt-config` also removes the repo, so that saves me another step."},{"id":"/2022/01/17/installing-openvz-6","metadata":{"permalink":"/blog/2022/01/17/installing-openvz-6","editUrl":"https://github.com/wpyoga/wpyoga.github.io/edit/master/blog/2022-01-17-installing-openvz-6.md","source":"@site/blog/2022-01-17-installing-openvz-6.md","title":"Installing OpenVZ 6","description":"OpenVZ 6 is an obsolete piece of technology. However, I have a small project whereby I upgrade a Ubuntu 16.04 installation to Ubuntu 18.04 and then 20.04 and eventually 22.04, completely disregarding any potential issues and problems that may happen :)","date":"2022-01-17T00:00:00.000Z","formattedDate":"January 17, 2022","tags":[{"label":"openvz","permalink":"/blog/tags/openvz"},{"label":"centos","permalink":"/blog/tags/centos"},{"label":"container","permalink":"/blog/tags/container"},{"label":"virtualization","permalink":"/blog/tags/virtualization"}],"truncated":true,"authors":[],"prevItem":{"title":"Deciphering Aptitude Search Results","permalink":"/blog/2022/01/19/deciphering-aptitude-search-results"},"nextItem":{"title":"Enabling IPv6 on Oracle Cloud Instance","permalink":"/blog/2021/10/30/oracle-cloud-instance-ipv6"}},"content":"OpenVZ 6 is an obsolete piece of technology. However, I have a small project whereby I upgrade a Ubuntu 16.04 installation to Ubuntu 18.04 and then 20.04 and eventually 22.04, completely disregarding any potential issues and problems that may happen :)\\n\\nAlso, I may be able to research a migration strategy for OpenVZ 6 to KVM. One possible way is to install OpenVZ 6 inside a KVM virtual machine, and then migrating each container as a single container inside the new host.\\n\\nThe basic process is to install CentOS 6 as the base OS, then install OpenVZ 6 on top of it. After installation, we have to reboot into the newly-installed OpenVZ kernel. This is a modified kernel based off of RHEL 6 / CentOS 6 that provides the containerization features.\\n\\nAs a bonus, I\'m installing all of this inside a KVM virtual machine.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Install CentOS 6\\n\\nTODO: move this to another document \\"Installing CentOS 6 in 2022\\".\\n\\ndownload centos 6:\\nhttps://vault.centos.org/6.10/isos/x86_64/\\nhttp://ftp.uem.br/linux/CentOS/6.10/isos/x86_64/\\nhttps://mirror.nsc.liu.se/centos-store/6.10/isos/x86_64/\\nhttp://ftp.iij.ad.jp/pub/linux/centos-vault/centos/6.10/isos/x86_64/\\nhttps://img.cs.montana.edu/linux/centos/6.10/isos/x86_64/\\n\\ninstall centos 6\\nenable dhcp: https://www.serverlab.ca/tutorials/linux/administration-linux/configure-centos-6-network-settings/\\n(just change ONBOOT=yes)\\n\\nuse vault repo (since centos 6 is already EOL)\\nhttps://www.getpagespeed.com/server-setup/how-to-fix-yum-after-centos-6-went-eol\\nedit /etc/yum.repos.d/CentOS-Base.repo\\nremove all mirrorlist= lines\\nuncomment all baseurl= lines, change the domain to vault.centos.org\\n```\\n$ sudo sed -i /etc/yum.repos.d/CentOS-Base.repo -e \'/^mirrorlist=/d\' -e \'s,^#baseurl=http://mirror.centos.org/,baseurl=https://vault.centos.org/,\'\\n```\\n\\nyum update\\n\\nmake a new user\\n```\\n# useradd william -G adm\\n```\\n\\nchange hostname\\n```\\n$ sudo hostname cos6\\n$ vi /etc/sysconfig/network\\nHOSTNAME=cos6\\n```\\n\\nenable sudo\\n$ visudo william\\nadd the line\\nwilliam ALL=(ALL)   NOPASSWD:ALL\\n\\n\\nenable ssh\\n$ mkdir /home/william/.ssh\\n$ vi /home/william/.ssh/authorized_keys\\n$ chmod 700 /home/william/.ssh\\n$ chown -R william: /home/william\\n$ ls -la /home/william/.ssh\\n\\ninstall & enable acpid, so that the VM can be shut down gracefully by libvirt / proxmox\\n```\\n$ sudo yum install acpid\\n$ sudo service acpid start\\n$ sudo service acpid status\\n```\\n\\n## Install OpenVZ 6\\n\\n\\nthen follow the instructions here:\\nhttps://wiki.openvz.org/Quick_Installation_CentOS_6\\n\\n$ sudo curl http://download.openvz.org/openvz.repo -o/etc/yum.repos.d/openvz.repo\\n$ sudo rpm --import http://download.openvz.org/RPM-GPG-Key-OpenVZ\\n$ yum install vzkernel vzctl vzquota\\n\\n# disable disk quota system\\n/etc/vz/vz.conf\\nDISK_QUOTA=no\\n\\n\\n# you can install ploop, but it is not recommended\\n/etc/vz/vz.conf\\nVE_LAYOUT=simfs\\n\\nmake sure vzkernel is the default boot option\\n$ sudo vi /boot/grub/menu.lst\\n\\nenable ip forwarding etc\\n$ sudo vi /etc/sysctl.conf\\nnet.ipv4.ip_forward = 1\\nnet.ipv4.conf.default.proxy_arp = 0\\nnet.ipv4.conf.all.rp_filter = 1\\n#kernel.sysrq = 1\\nnet.ipv4.conf.default.send_redirects = 1\\nnet.ipv4.conf.all.send_redirects = 0\\nnet.ipv4.icmp_echo_ignore_broadcasts=1\\nnet.ipv4.conf.default.forwarding=1\\n\\n### If using veth\\n\\nfor VETH: create a bridge & configure vznet script\\n$ sudo vi /etc/sysconfig/network-scripts/ifcfg-vmbr0\\nDEVICE=\\"vmbr0\\"\\nBOOTPROTO=\\"static\\"\\nIPV6INIT=\\"no\\"\\nONBOOT=\\"yes\\"\\nTYPE=\\"Bridge\\"\\nDELAY=0\\nIPADDR=192.168.124.99\\nNETMASK=255.255.255.0\\nGATEWAY=192.168.124.1\\n$ sudo vi /etc/sysconfig/network-scripts/ifcfg-eth0\\nDEVICE=eth0\\nTYPE=Ethernet\\nONBOOT=yes\\nIPV6INIT=no\\nBRIDGE=vmbr0\\n$ sudo vi /etc/vz/vznet.conf \\nEXTERNAL_SCRIPT=/usr/sbin/vznetaddbr\\n\\n### Create a container\\n\\ncreate container\\n$ sudo vzctl create 101 --ostemplate ubuntu-16.04-x86_64 --layout simfs --hostname server101 --ipadd 192.168.124.101 --diskspace 8G\\n$ sudo vzctl set 101 --save --onboot yes --nameserver 8.8.8.8 --cpus 1 --ram 1G\\n\\n# use --name server101 ???\\n\\nnote: if using ploop, sometimes container can run out of disk space, especially when there is lots of writing and erasing. to solve this: ploop discard -d /dev/ploopXXXXX / or / vzctl compact 101\\nit\'s much simpler to just use simfs though\\n\\nconfigure venet: https://wiki.openvz.org/Virtual_network_device\\n\\n$ sudo vzctl start 101\\n\\n# ubuntu 16.04 template\\n# ubuntu useradd needs extra options\\n$ sudo vzctl exec 101 useradd -m -s /bin/bash william\\n$ sudo vzctl exec 101 usermod -a -G systemd-journal william\\n$ sudo vzctl exec 101 mkdir /home/william/.ssh\\n$ sudo vi /vz/root/101/home/william/.ssh/authorized_keys\\n$ sudo vzctl exec 101 chmod 700 /home/william/.ssh\\n$ sudo vzctl exec 101 chown -R william: /home/william\\n$ sudo vzctl exec 101 ls -la /home/william/.ssh\\n\\n$ sudo vi /vz/root/101/etc/sudoers\\nwilliam ALL=(ALL:ALL) NOPASSWD: ALL\\n\\nlogin to the node, then\\n$ sudo dpkg-reconfigure locales\\nselect en_US.UTF-8 to generate\\nand select en_US.UTF-8 as the system default locale\\n\\n---\\n\\nto list all containers: vzlist --all\\n\\n---\\n\\n### Configure networking\\n\\n\\non the openvz host\\n$ sudo iptables -D FORWARD -j REJECT --reject-with icmp-host-prohibited\\n\\n/etc/sysconfig/iptables\\nremove the line:\\n```\\n-A FORWARD -j REJECT --reject-with icmp-host-prohibited\\n```\\n\\non the physical host\\n```\\n$ sudo ip route add 192.168.124.0/24 via 192.168.122.223\\n```\\n\\nor:\\n\\n$ virsh net-edit default && virsh net-destroy default && virsh net-start default && sudo systemctl restart libvirtd\\n```xml\\n<network>\\n  ...\\n  <route family=\'ipv4\' address=\'192.168.124.0\' prefix=\'24\' gateway=\'192.168.122.223\'/>\\n</network>\\n```\\n\\n\\n\\n\\n# on the physical host: copy libvirt rules\\n# the last rule is actually the most important one\\nsudo iptables -D LIBVIRT_FWI -o virbr0 -j REJECT --reject-with icmp-port-unreachable\\nsudo iptables -I FORWARD -s 192.168.124.0/24 -i virbr0 -j ACCEPT\\nsudo iptables -I FORWARD -d 192.168.124.0/24 -o virbr0 -j ACCEPT\\nsudo iptables -t nat -A POSTROUTING -s 192.168.124.0/24 ! -d 192.168.124.0/24 -j MASQUERADE\\n\\ninstall iptables-persistent\\nthen create /etc/iptables/rules.v4\\n```\\n*filter\\n-A FORWARD -d 192.168.124.0/24 -o virbr0 -j ACCEPT\\n-A FORWARD -s 192.168.124.0/24 -i virbr0 -j ACCEPT\\nCOMMIT\\n\\n*nat\\n-A POSTROUTING -s 192.168.124.0/24 ! -d 192.168.124.0/24 -j MASQUERADE\\nCOMMIT\\n```\\n\\ndon\'t save the libvirt and wireguard rules to that file, those rules will be recreated when the corresponding services are started"},{"id":"/2021/10/30/oracle-cloud-instance-ipv6","metadata":{"permalink":"/blog/2021/10/30/oracle-cloud-instance-ipv6","editUrl":"https://github.com/wpyoga/wpyoga.github.io/edit/master/blog/2021-10-30-oracle-cloud-instance-ipv6.md","source":"@site/blog/2021-10-30-oracle-cloud-instance-ipv6.md","title":"Enabling IPv6 on Oracle Cloud Instance","description":"I have a couple of always-free compute instances on Oracle Cloud. By default, each instance gets an IPv4 address, and IPv6 is not enabled.","date":"2021-10-30T00:00:00.000Z","formattedDate":"October 30, 2021","tags":[{"label":"oracle","permalink":"/blog/tags/oracle"},{"label":"cloud","permalink":"/blog/tags/cloud"},{"label":"instance","permalink":"/blog/tags/instance"},{"label":"oci","permalink":"/blog/tags/oci"},{"label":"ipv6","permalink":"/blog/tags/ipv-6"},{"label":"firewall","permalink":"/blog/tags/firewall"},{"label":"firewalld","permalink":"/blog/tags/firewalld"},{"label":"dhcp","permalink":"/blog/tags/dhcp"}],"truncated":true,"authors":[],"prevItem":{"title":"Installing OpenVZ 6","permalink":"/blog/2022/01/17/installing-openvz-6"},"nextItem":{"title":"Using DNF on an IPv6-only host","permalink":"/blog/2021/10/26/dnf-ipv6"}},"content":"I have a couple of always-free compute instances on Oracle Cloud. By default, each instance gets an IPv4 address, and IPv6 is not enabled.\\n\\nI tried enabling IPv6 using [instructions I found on the Internet](https://blog.51sec.org/2021/09/enable-ipv6-on-oracle-cloud.html), but:\\n- it doesn\'t work for my Oracle Linux 8.4 instance\\n- even if it did, we have to manually obtain the IPv6 address using `/etc/rc.local` -- this doesn\'t seem to be the best practice, especially since this is a systemd distro\\n\\nHey, I\'m all for simplicity and I love SysV init scripts, but if the distro is a systemd distro, then I\'ll use systemd.\\nJust like the reason why I have an Oracle Linux instance on Oracle Cloud -- it\'s supposed to be the best-supported OS on their cloud.\\nYes, it\'s just another clone of RHEL, but I thought it would somehow be better tuned for Oracle Cloud environment...\\n\\n\x3c!-- truncate --\x3e\\n\\nAnyway, I do have a Ubuntu instance on Oracle Cloud, and it\'s much easier to set up and figure out as you can see below.\\n\\n## Enable and assign IPv6 to the instance\\n\\nThis is done from on the cloud infrastructure, and can be easily done using the [Cloud Console](https://cloud.oracle.com/). These instructions are taken from [the aforementioned article](https://blog.51sec.org/2021/09/enable-ipv6-on-oracle-cloud.html).\\n\\nOnce logged in to the Cloud Console, open the instance to be configured and perform the following actions:\\n\\n1. Configure VCN:\\n\\n    1. CIDR Blocks: add an IPv6 CIDR block\\n\\n    1. Subnets: Edit -> enable the CIDR block\\n\\n1. Configure Security List:\\n\\n    - Ingress Rules: add IPv6-ICMP type 128 to allow incoming ping (from `::/0`)\\n\\n    - Egress Rules: allow traffic of all protocols to all destinations (`::/0`)\\n\\n1. Configure Route Table: Route Rules -> add IPv6 route rule to `::/0` targeting the default Internet Gateway\\n\\n1. Configure VNIC: IPv6 Addresses -> assign an IPv6 address\\n\\nAt this point, an IPv6 address has been assigned to the instance, and the network has been set up properly.\\n\\nIf you add new instances into the same VCN, then you would only need to assign a new IPv6 address to the VNIC, no need to reconfigure the other resources (CIDR Blocks, subnets, security lists, route table).\\n\\n## Obtain the IPv6 address from the instance\\n\\nNext, obtain the IPv6 address from within the instance itself.\\n\\n### Oracle Linux 7.9\\n\\n`dhcpv6-client` service is already enabled by default.\\n\\nCreate a new file `/etc/cloud/cloud.cfg.d/99_ipv6.cfg`:\\n\\n```yaml\\nnetwork:\\n  version: 2\\n  ethernets:\\n    enp0s3:\\n      dhcp: true\\n      dhcp6: true\\n```\\n\\nTODO: somehow my Ampere instance doesn\'t work yet\\n\\n### Oracle Linux 8\\n\\nSSH into the instance, and [add DHCPv6 service](https://docs.oracle.com/en-us/iaas/Content/Network/Concepts/ipv6.htm#os_config) to the firewall:\\n\\n```shell-session\\n$ sudo firewall-cmd --add-service=dhcpv6-client\\n```\\n\\nCheck that the firewall setting have been applied properly:\\n\\n```shell-session\\n$ sudo firewall-cmd --list-all\\n```\\n\\nTell NetworkManager to obtain the IPv6 address:\\n\\n```shell-session\\n$ sudo systemctl restart NetworkManager\\n```\\n\\nWait a few seconds, and then check to see that the IPv6 address has been obtained successfully.\\nWe need to wait a few seconds because dhclient needs some time to obtain and then apply the IPv6 address.\\n\\n```shell-session\\n$ ip add\\n```\\n\\nIf all is well, make the firewall rule permanent:\\n\\n```shell-session\\n$ sudo firewall-cmd --add-service=dhcpv6-client --permanent\\n```\\n\\nI actually wasted a few hours trying to find this information.\\nIt turns out DHCP was blocked by default and we need to allow it.\\nNot sure why most people recommend setting the IPv6 statically...\\n\\nMoral of the story: read the official documentation!\\n\\n### Ubuntu\\n\\nSSH into the instance, and reload the network configuration:\\n\\n```shell-session\\n$ sudo systemctl restart systemd-networkd\\n```\\n\\nThat\'s it. It\'s much simpler isn\'t it? :)\\n\\nP.S. on my Ampere instance, the IPv6 is applied almost as soon as I configured it."},{"id":"/2021/10/26/dnf-ipv6","metadata":{"permalink":"/blog/2021/10/26/dnf-ipv6","editUrl":"https://github.com/wpyoga/wpyoga.github.io/edit/master/blog/2021-10-26-dnf-ipv6.md","source":"@site/blog/2021-10-26-dnf-ipv6.md","title":"Using DNF on an IPv6-only host","description":"I created a Stardust instance on ScaleWay, and used IPv6 exclusively (didn\'t use IPv4, much cheaper this way).","date":"2021-10-26T00:00:00.000Z","formattedDate":"October 26, 2021","tags":[{"label":"dnf","permalink":"/blog/tags/dnf"},{"label":"ipv6","permalink":"/blog/tags/ipv-6"}],"truncated":true,"authors":[],"prevItem":{"title":"Enabling IPv6 on Oracle Cloud Instance","permalink":"/blog/2021/10/30/oracle-cloud-instance-ipv6"},"nextItem":{"title":"Vue.js Component vs HTML Tag Names","permalink":"/blog/2021/10/15/vue-component-tag-name"}},"content":"I created a Stardust instance on ScaleWay, and used IPv6 exclusively (didn\'t use IPv4, much cheaper this way).\\n\\nThe issue is, `dnf` would often not work. After using `-d 3` to figure out where it stalls, it became clear that `dnf` was resolving the mirror domain names to IPv4, and then failing to connect to them because we don\'t have IPv4.\\n\\n\x3c!-- truncate --\x3e\\n\\nThe solution is to edit `/etc/dnf/dnf.conf` and add this line:\\n\\n```conf\\nip_resolve=6\\n```\\n\\nFor other options, read the man page of `dnf.conf`.\\n\\nThere was some effort to enable a drop-in configuration diretory `/etc/dnf/conf.d`, but the patch never got merged.\\n- bugzilla: https://bugzilla.redhat.com/show_bug.cgi?id=1352234\\n- pull request: https://github.com/rpm-software-management/dnf/pull/887\\n\\nAnyway, after reconfiguring DNF to resolve domain names to IPv6 only, it works fine."},{"id":"/2021/10/15/vue-component-tag-name","metadata":{"permalink":"/blog/2021/10/15/vue-component-tag-name","editUrl":"https://github.com/wpyoga/wpyoga.github.io/edit/master/blog/2021-10-15-vue-component-tag-name.md","source":"@site/blog/2021-10-15-vue-component-tag-name.md","title":"Vue.js Component vs HTML Tag Names","description":"So, here\'s a snippet taken from a Vue.js project:","date":"2021-10-15T00:00:00.000Z","formattedDate":"October 15, 2021","tags":[{"label":"vue","permalink":"/blog/tags/vue"},{"label":"component","permalink":"/blog/tags/component"},{"label":"html","permalink":"/blog/tags/html"},{"label":"tag","permalink":"/blog/tags/tag"},{"label":"pascalcase","permalink":"/blog/tags/pascalcase"},{"label":"kebabcase","permalink":"/blog/tags/kebabcase"}],"truncated":false,"authors":[],"prevItem":{"title":"Using DNF on an IPv6-only host","permalink":"/blog/2021/10/26/dnf-ipv6"},"nextItem":{"title":"Fast Loading for `nvm` and SDKMAN!","permalink":"/blog/2021/07/21/nvm-sdkman-fast-loading"}},"content":"So, here\'s a snippet taken from a Vue.js project:\\n\\n```js\\nexport default {\\n\\tname: \'Room\',\\n\\tcomponents: {\\n\\t\\tInfiniteLoading,\\n\\t\\tLoader,\\n\\t\\tSvgIcon,\\n        ...\\n```\\n\\nBut we don\'t see `SvgIcon` anywhere in the HTML template. We see this instead:\\n\\n```html\\n    <svg-icon name=\\"emoji\\" :param=\\"emojiReaction ? \'reaction\' : \'\'\\" />\\n```\\n\\nHow does `SvgIcon` become `svg-icon`? By magic, it [seems](https://vuejs.org/v2/guide/components-registration.html#Name-Casing).\\n\\nOh, how I love C and its (almost) no-surprises syntax!"},{"id":"/2021/07/21/nvm-sdkman-fast-loading","metadata":{"permalink":"/blog/2021/07/21/nvm-sdkman-fast-loading","editUrl":"https://github.com/wpyoga/wpyoga.github.io/edit/master/blog/2021-07-21-nvm-sdkman-fast-loading.md","source":"@site/blog/2021-07-21-nvm-sdkman-fast-loading.md","title":"Fast Loading for `nvm` and SDKMAN!","description":"Some time ago I was trying to make bash start faster. There, I described a solution to load nvm and SDKMAN! faster. However, that solution has a few issues.","date":"2021-07-21T00:00:00.000Z","formattedDate":"July 21, 2021","tags":[{"label":"bash","permalink":"/blog/tags/bash"},{"label":"shell","permalink":"/blog/tags/shell"},{"label":"nvm","permalink":"/blog/tags/nvm"},{"label":"sdkman","permalink":"/blog/tags/sdkman"},{"label":"slow","permalink":"/blog/tags/slow"}],"truncated":true,"authors":[],"prevItem":{"title":"Vue.js Component vs HTML Tag Names","permalink":"/blog/2021/10/15/vue-component-tag-name"},"nextItem":{"title":"Bash Shell Slow Start-up","permalink":"/blog/2021/07/11/bash-slow-start"}},"content":"Some time ago I was trying to [make bash start faster](/blog/2021/07/10/bashrc-directory#self-cleaning-implementation). There, I described a solution to load `nvm` and SDKMAN! faster. However, that solution has a few issues.\\n\\n\x3c!-- truncate --\x3e\\n\\n- we always need to call one of `nvm`, `node`, or `npm` before we can call any other node.js-related command\\n- we can avoid the previous problem by manually adding them to `50-nvm.bashrc`, but this is error-prone and cumbersome\\n- when we add a new utility, we need to manually add them to the list, and to every function there\\n- if we switch the default node.js version, then we need to re-evaluate the whole script. for example, our node.js 12 installation has `yarn` installed, but our node.js 14 installation doesn\'t have it installed. we would then need to remove `yarn` from the bashrc script\\n\\nTo solve this, we need something that:\\n\\n- can automatically find the currently-used node version\\n- can autogenerate the list and the functions, according to which utilities are installed\\n\\n## The solution\\n\\nAfter some thinking and experimentation, I have come up with a new solution:\\n\\n```shell title=\\".bashrc.d/50-nvm.bashrc\\"\\n_DEFAULT=\\n# read the first line only\\n[ -s \\"${HOME}/.nvm/alias/default\\" ] && read -r _DEFAULT <\\"${HOME}/.nvm/alias/default\\"\\n\\nif [ -n \\"${_DEFAULT}\\" ]; then\\n  # this will simplify the checks below\\n  _DEFAULT=${_DEFAULT#v}\\n  _DEFAULT=${_DEFAULT%.}\\n\\n  if [ -d \\"${HOME}/.nvm/versions/node/v${_DEFAULT}/bin\\" ]; then\\n    _DEFAULT_DIR=\\"${HOME}/.nvm/versions/node/v${_DEFAULT}/bin\\"\\n  elif [ -d \\"${HOME}/.nvm/versions/node/v${_DEFAULT}\\".*/bin ]; then\\n    _DEFAULT_DIR=\\"$(echo \\"${HOME}/.nvm/versions/node/v${_DEFAULT}\\".*/bin)\\"\\n  else\\n    _DEFAULT_DIR=\\n  fi\\n\\n  _NVM_BIN_LIST=\\n  if [ -n \\"${_DEFAULT_DIR}\\" ]; then\\n    for _EXEC in \\"${_DEFAULT_DIR}\\"/*; do\\n      if [ -x \\"${_EXEC}\\" ]; then\\n        _CMD=\\"${_EXEC##*/}\\"\\n        eval \\"${_CMD}\\"\'() { _nvm_load; \'\\"${_CMD}\\"\' \\"$@\\"; }\'\\n        _NVM_BIN_LIST=\\"${_NVM_BIN_LIST} ${_CMD}\\"\\n      fi\\n    done\\n  fi\\nfi\\n\\n_CMD=\'nvm\'\\neval \\"${_CMD}\\"\'() { _nvm_load; \'\\"${_CMD}\\"\' \\"$@\\"; }\'\\n\\neval \'_nvm_load() {\\n  unset -f _nvm_load nvm \'\\"${_NVM_BIN_LIST}\\"\'\\n  export NVM_DIR=\\"${HOME}/.nvm\\"\\n  [ -s \\"${NVM_DIR}/nvm.sh\\" ] && . \\"${NVM_DIR}/nvm.sh\\"\\n  [ -s \\"${NVM_DIR}/bash_completion\\" ] && . \\"${NVM_DIR}/bash_completion\\"\\n}\'\\n\\nunset _CMD _NVM_BIN_LIST _EXEC _DEFAULT_DIR _DEFAULT\\n```\\n\\nWhen the script is sourced in by `.bashrc`, it will first try to read the default version. This is usually read in as something like `12` or `12.22`. With that version information, it tries to find the corresponding nvm directory and list all the executables there. For each executable, a placeholder shell function is created. When we first call the placeholder, it will initialize `nvm`, after which everything will return to normal.\\n\\nThis is the environment on my laptop:\\n\\n```shell-session\\nwilliam@william-ThinkPad-T450s: ~$ cat .nvm/alias/default\\n12\\nwilliam@william-ThinkPad-T450s: ~$ ls -l .nvm/versions/node/v12*/bin/*\\n-rwxr-xr-x 1 william william 48928552 Apr  6 22:06 .nvm/versions/node/v12.22.1/bin/node\\nlrwxrwxrwx 1 william william       42 May 17 13:12 .nvm/versions/node/v12.22.1/bin/nodemon -> ../lib/node_modules/nodemon/bin/nodemon.js\\nlrwxrwxrwx 1 william william       38 Jun 21 01:34 .nvm/versions/node/v12.22.1/bin/npm -> ../lib/node_modules/npm/bin/npm-cli.js\\nlrwxrwxrwx 1 william william       38 Jun 21 01:34 .nvm/versions/node/v12.22.1/bin/npx -> ../lib/node_modules/npm/bin/npx-cli.js\\nlrwxrwxrwx 1 william william       36 Jun 10 22:51 .nvm/versions/node/v12.22.1/bin/yarn -> ../lib/node_modules/yarn/bin/yarn.js\\nlrwxrwxrwx 1 william william       36 Jun 10 22:51 .nvm/versions/node/v12.22.1/bin/yarnpkg -> ../lib/node_modules/yarn/bin/yarn.js\\nlrwxrwxrwx 1 william william       33 May 27 20:20 .nvm/versions/node/v12.22.1/bin/yo -> ../lib/node_modules/yo/lib/cli.js\\nlrwxrwxrwx 1 william william       46 May 27 20:20 .nvm/versions/node/v12.22.1/bin/yo-complete -> ../lib/node_modules/yo/lib/completion/index.js\\nwilliam@william-ThinkPad-T450s: ~$ type _nvm_load nvm node npm yarn\\n_nvm_load is a function\\n_nvm_load () \\n{ \\n    unset -f _nvm_load nvm node nodemon npm npx yarn yarnpkg yo yo-complete;\\n    export NVM_DIR=\\"${HOME}/.nvm\\";\\n    [ -s \\"${NVM_DIR}/nvm.sh\\" ] && . \\"${NVM_DIR}/nvm.sh\\";\\n    [ -s \\"${NVM_DIR}/bash_completion\\" ] && . \\"${NVM_DIR}/bash_completion\\"\\n}\\nnvm is a function\\nnvm () \\n{ \\n    _nvm_load;\\n    nvm \\"$@\\"\\n}\\nnode is a function\\nnode () \\n{ \\n    _nvm_load;\\n    node \\"$@\\"\\n}\\nnpm is a function\\nnpm () \\n{ \\n    _nvm_load;\\n    npm \\"$@\\"\\n}\\nyarn is a function\\nyarn () \\n{ \\n    _nvm_load;\\n    yarn \\"$@\\"\\n}\\n```\\n\\nThe placeholder function for `yo` is there, I just didn\'t show it.\\n\\n## Loading time\\n\\nLoading time is still fast enough:\\n\\n```shell-session\\n$ time . .bashrc.d/50-nvm.bashrc \\n\\nreal\\t0m0.002s\\nuser\\t0m0.001s\\nsys\\t0m0.001s\\n```\\n\\n## Testing\\n\\nWith this loading script, `nvm` still functions correctly:\\n\\n```shell-session\\n$ yarn --version\\n1.22.10\\n```\\n\\n```shell-session\\n$ node --version\\nv12.22.1\\n```\\n\\n```shell-session\\n$ nvm --version\\n0.38.0\\n```\\n\\n```shell-session\\n$ npm --version\\n7.18.1\\n```\\n\\nAlso, it still respects `.nvmrc` as described [here](https://github.com/nvm-sh/nvm#nvmrc):\\n\\n```shell-session\\nwilliam@william-ThinkPad-T450s: ~$ mkdir test\\nwilliam@william-ThinkPad-T450s: ~$ echo 14 >test/.nvmrc\\nwilliam@william-ThinkPad-T450s: ~$ mkdir test/foo\\nwilliam@william-ThinkPad-T450s: ~$ cd test/foo\\nwilliam@william-ThinkPad-T450s: ~/test/foo$ nvm use\\nFound \'/home/william/test/.nvmrc\' with version <14>\\nNow using node v14.17.1 (npm v7.19.0)\\nwilliam@william-ThinkPad-T450s: ~/test/foo$ cd ..\\nwilliam@william-ThinkPad-T450s: ~/test$ nvm use\\nFound \'/home/william/test/.nvmrc\' with version <14>\\nNow using node v14.17.1 (npm v7.19.0)\\nwilliam@william-ThinkPad-T450s: ~/test$ cd ..\\nwilliam@william-ThinkPad-T450s: ~$ nvm use\\nNo .nvmrc file found\\nPlease see `nvm --help` or https://github.com/nvm-sh/nvm#nvmrc for more information.\\n```\\n\\n## Shortcomings\\n\\nOf course, no solution is perfect. Otherwise the `nvm` developers would have adopted it as their official solution/workaround to the slow loading times of `nvm`. However, there is currently one way that I can think of, to make this solution fail: install a new utility in another terminal session.\\n\\nIn this scenario, we first open a terminal window, but don\'t call any `nvm` or node.js-related command. Let\'s say we don\'t have `yarn` installed at this point. Then in another terminal window, we install `yarn`. Now, in the first terminal window, we cannot execute `yarn` without first calling `nvm` or any other existing commands.\\n\\nThe reverse is not a problem. Let\'s say we have `yo` installed. If we uninstall `yo` in the other terminal window, when we try to execute it in the first terminal window, it will just fail. This is the exact same behaviour as a vanilla `nvm` installation.\\n\\nPlease let me know if you have any other potential pitfalls and shortcomings\\n\\n## What about SDKMAN! ?\\n\\nWell, SDKMAN! is a bit simpler than `nvm`:\\n\\n```shell title=\\".bashrc.d/50-sdkman.bashrc\\"\\n_SDK_BIN_LIST=\\nfor _EXEC in \\"${HOME}/.sdkman/candidates\\"/*/current/bin/*; do\\n  if [ -x \\"${_EXEC}\\" ]; then\\n    _CMD=\\"${_EXEC##*/}\\"\\n    eval \\"${_CMD}\\"\'() { _sdk_load; \'\\"${_CMD}\\"\' \\"$@\\"; }\'\\n    _SDK_BIN_LIST=\\"${_SDK_BIN_LIST} ${_CMD}\\"\\n  fi\\ndone\\n\\n_CMD=\'sdk\'\\neval \\"${_CMD}\\"\'() { _sdk_load; \'\\"${_CMD}\\"\' \\"$@\\"; }\'\\n\\neval \'_sdk_load() {\\n  unset -f _sdk_load sdk \'\\"${_SDK_BIN_LIST}\\"\'\\n  export SDKMAN_DIR=\\"${HOME}/.sdkman\\"\\n  [ -s \\"${SDKMAN_DIR}/bin/sdkman-init.sh\\" ] && . \\"${SDKMAN_DIR}/bin/sdkman-init.sh\\"\\n}\'\\n\\nunset _CMD _SDK_BIN_LIST _EXEC\\n```\\n\\nSince SDKMAN! uses a `current` symlink for each installed candidate, we can just use this. No need to manually figure out the currently-used version.\\n\\nThe solution for SDKMAN! suffers the same shortcoming as the one for `nvm`, so please also let me know if you find something wrong with it."},{"id":"/2021/07/11/bash-slow-start","metadata":{"permalink":"/blog/2021/07/11/bash-slow-start","editUrl":"https://github.com/wpyoga/wpyoga.github.io/edit/master/blog/2021-07-11-bash-slow-start.md","source":"@site/blog/2021-07-11-bash-slow-start.md","title":"Bash Shell Slow Start-up","description":"As we install development environments in our system, we may notice that terminal start-up can become slow. Some people notice it, some don\'t -- but the reality is, it\'s there.","date":"2021-07-11T00:00:00.000Z","formattedDate":"July 11, 2021","tags":[{"label":"bash","permalink":"/blog/tags/bash"},{"label":"shell","permalink":"/blog/tags/shell"},{"label":"slow","permalink":"/blog/tags/slow"},{"label":"nvm","permalink":"/blog/tags/nvm"},{"label":"sdkman","permalink":"/blog/tags/sdkman"}],"truncated":true,"authors":[],"prevItem":{"title":"Fast Loading for `nvm` and SDKMAN!","permalink":"/blog/2021/07/21/nvm-sdkman-fast-loading"},"nextItem":{"title":"Manual Profiling of Bash Script Execution","permalink":"/blog/2021/07/11/manual-profiling-bash-script"}},"content":"As we install development environments in our system, we may notice that terminal start-up can become slow. Some people notice it, some don\'t -- but the reality is, it\'s there.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Finding the culprit\\n\\nThis initially started when I was [looking at bash start-up time](/blog/2021/07/10/bashrc-directory#self-cleaning-implementation), and that\'s when I noticed that `nvm` loading was a bit slow.\\n\\nHere\'s how I found out, by printing the time taken to execute commands:\\n\\n```shell\\nfor i in \\"${HOME}/.bashrc.d\\"/[0-9][0-9]-*.bashrc; do\\n  if [ -r \\"$i\\" ]; then\\n    export TIMEFORMAT=\\"%R $i\\"\\n    time . \\"$i\\"\\n  fi\\ndone\\nunset i\\nunset TIMEFORMAT\\n```\\n\\nThat gives me this printout when I open a new terminal:\\n\\n```txt\\n0.051 /home/william/.bashrc.d/00-default.bashrc\\n0.000 /home/william/.bashrc.d/50-android-sdk.bashrc\\n0.000 /home/william/.bashrc.d/50-android-studio.bashrc\\n0.008 /home/william/.bashrc.d/50-asdf.bashrc\\n0.266 /home/william/.bashrc.d/50-nvm.bashrc\\n0.037 /home/william/.bashrc.d/50-sdkman.bashrc\\n0.000 /home/william/.bashrc.d/90-gradle-opts.bashrc\\n```\\n\\nThere is another thing that I should check:\\n\\n```shell\\nexport TIMEFORMAT=\'%R bashrc check\'\\ntime if which mktemp >/dev/null 2>&1; then\\n  ...\\n```\\n\\nWhich gives me:\\n\\n```txt\\n0.019 bashrc check\\n```\\n\\nSo, it seems that `nvm` takes the longest time, followed by the default bashrc and then `SDKMAN!`.\\n\\nThe problem with slow loading has been experienced by other people:\\n\\n- https://github.com/nvm-sh/nvm/issues/1261\\n- https://github.com/nvm-sh/nvm/issues/1277\\n\\nAlthough I\'m lucky to now have experienced such slow loading myself.\\n\\n## The solution\\n\\nJust like most good ideas, others have already thought about it:\\n\\n- https://gist.github.com/rtfpessoa/811701ed8fa642f60e411aef04b2b64a\\n- https://armno.in.th/2020/08/24/lazyload-nvm-to-reduce-zsh-startup-time/\\n- https://www.ioannispoulakas.com/2020/02/22/how-to-speed-up-shell-load-while-using-nvm/\\n\\nBut [this one](http://broken-by.me/lazy-load-nvm/) looks the most promising.\\n\\nSo I adapted it for my own use:\\n\\n```shell\\nnvm() {\\n  unset -f nvm node npm\\n  export NVM_DIR=\\"${HOME}/.nvm\\"\\n  [ -s \\"${NVM_DIR}/nvm.sh\\" ] && . \\"${NVM_DIR}/nvm.sh\\"\\n  [ -s \\"${NVM_DIR}/bash_completion\\" ] && . \\"${NVM_DIR}/bash_completion\\"\\n  nvm \\"$@\\"\\n}\\n\\nnode() {\\n  unset -f nvm node npm\\n  export NVM_DIR=\\"${HOME}/.nvm\\"\\n  [ -s \\"${NVM_DIR}/nvm.sh\\" ] && . \\"${NVM_DIR}/nvm.sh\\"\\n  [ -s \\"${NVM_DIR}/bash_completion\\" ] && . \\"${NVM_DIR}/bash_completion\\"\\n  node \\"$@\\"\\n}\\n\\nnpm() {\\n  unset -f nvm node npm\\n  export NVM_DIR=\\"${HOME}/.nvm\\"\\n  [ -s \\"${NVM_DIR}/nvm.sh\\" ] && . \\"${NVM_DIR}/nvm.sh\\"\\n  [ -s \\"${NVM_DIR}/bash_completion\\" ] && . \\"${NVM_DIR}/bash_completion\\"\\n  npm \\"$@\\"\\n}\\n```\\n\\nNow the timings have improved:\\n\\n```txt\\n0.034 /home/william/.bashrc.d/00-default.bashrc\\n0.000 /home/william/.bashrc.d/50-android-sdk.bashrc\\n0.000 /home/william/.bashrc.d/50-android-studio.bashrc\\n0.002 /home/william/.bashrc.d/50-asdf.bashrc\\n0.000 /home/william/.bashrc.d/50-nvm.bashrc\\n0.039 /home/william/.bashrc.d/50-sdkman.bashrc\\n0.000 /home/william/.bashrc.d/90-gradle-opts.bashrc\\n0.018 bashrc check\\n```\\n\\n## A little snag\\n\\nWith this setup, `yarn` cannot be called without calling `nvm` or `node` or `npm` first, because it\'s not in the PATH.\\n\\nNo solutions for this yet -- maybe we can do some kind of late-loading instead of lazy-loading.\\n\\nOr maybe, we can make nvm load faster... somehow?\\n\\n## Anything else?\\n\\nLet\'s use the same technique for SDKMAN! as well then:\\n\\n```shell\\nsdk() {\\n  unset -f sdk\\n  export SDKMAN_DIR=\\"${HOME}/.sdkman\\"\\n  [ -s \\"${SDKMAN_DIR}/bin/sdkman-init.sh\\" ] && . \\"${SDKMAN_DIR}/bin/sdkman-init.sh\\"\\n  sdk \\"$@\\"\\n}\\n```\\n\\nWith this, we have shaved more time off our start-up:\\n\\n```txt\\n0.060 /home/william/.bashrc.d/00-default.bashrc\\n0.000 /home/william/.bashrc.d/50-android-sdk.bashrc\\n0.000 /home/william/.bashrc.d/50-android-studio.bashrc\\n0.004 /home/william/.bashrc.d/50-asdf.bashrc\\n0.000 /home/william/.bashrc.d/50-nvm.bashrc\\n0.000 /home/william/.bashrc.d/50-sdkman.bashrc\\n0.000 /home/william/.bashrc.d/90-gradle-opts.bashrc\\n0.018 bashrc check\\n```\\n\\nGreat, now we just have to worry about the default `.bashrc` and the checking part :)\\n\\nBut that\'s another matter for another day."},{"id":"/2021/07/11/manual-profiling-bash-script","metadata":{"permalink":"/blog/2021/07/11/manual-profiling-bash-script","editUrl":"https://github.com/wpyoga/wpyoga.github.io/edit/master/blog/2021-07-11-manual-profiling-bash-script.md","source":"@site/blog/2021-07-11-manual-profiling-bash-script.md","title":"Manual Profiling of Bash Script Execution","description":"Sometimes, we would like to know how long a command takes, and which part of the script takes the longest time to run.","date":"2021-07-11T00:00:00.000Z","formattedDate":"July 11, 2021","tags":[{"label":"bash","permalink":"/blog/tags/bash"},{"label":"profiling","permalink":"/blog/tags/profiling"}],"truncated":true,"authors":[],"prevItem":{"title":"Bash Shell Slow Start-up","permalink":"/blog/2021/07/11/bash-slow-start"},"nextItem":{"title":"Using a `.bashrc.d` directory instead of just `.bashrc`","permalink":"/blog/2021/07/10/bashrc-directory"}},"content":"Sometimes, we would like to know how long a command takes, and which part of the script takes the longest time to run.\\n\\n\x3c!-- truncate --\x3e\\n\\n## The method\\n\\nWe can use `time`, which is a special bash keyword. Instead of using a separate `/usr/bin/time` command, it can be less overhead to use the keyword.\\n\\nFollowing the implementation [here](/blog/2021/07/10/bashrc-directory#self-cleaning-implementation), we add some profiling info to `~/.bashrc`:\\n\\n```shell\\nfor i in \\"${HOME}/.bashrc.d\\"/[0-9][0-9]-*.bashrc; do\\n  if [ -r \\"$i\\" ]; then\\n    export TIMEFORMAT=\\"%R $i\\"\\n    time . \\"$i\\"\\n  fi\\ndone\\nunset i\\n```\\n\\nThis showed that `nvm` makes bash load slowly:\\n\\n```\\n0.034 /home/william/.bashrc.d/00-default.bashrc\\n0.000 /home/william/.bashrc.d/50-android-sdk.bashrc\\n0.000 /home/william/.bashrc.d/50-android-studio.bashrc\\n0.002 /home/william/.bashrc.d/50-asdf.bashrc\\n0.261 /home/william/.bashrc.d/50-nvm.bashrc\\n0.036 /home/william/.bashrc.d/50-sdkman.bashrc\\n0.000 /home/william/.bashrc.d/90-gradle-opts.bashrc\\n```\\n\\n## An unforeseen problem\\n\\nSo, I began timing its execution:\\n\\n```shell\\nnvm_process_parameters() {\\n  local NVM_AUTO_MODE\\n  NVM_AUTO_MODE=\'use\'\\n  while [ $# -ne 0 ]; do\\n    case \\"$1\\" in\\n      --install) NVM_AUTO_MODE=\'install\' ;;\\n      --no-use) NVM_AUTO_MODE=\'none\' ;;\\n    esac\\n    shift\\n  done\\n  export TIMEFORMAT=\'%R nvm_auto\'\\n  time nvm_auto \\"${NVM_AUTO_MODE}\\"\\n}\\n\\nnvm_process_parameters \\"$@\\"\\n```\\n\\nalso:\\n\\n```shell\\nnvm_auto() {\\n  echo nvm_auto \\"$@\\"\\n  local NVM_MODE\\n  NVM_MODE=\\"${1-}\\"\\n  local VERSION\\n  local NVM_CURRENT\\n  if [ \\"_${NVM_MODE}\\" = \'_install\' ]; then\\n    VERSION=\\"$(nvm_alias default 2>/dev/null || nvm_echo)\\"\\n    if [ -n \\"${VERSION}\\" ]; then\\n      nvm install \\"${VERSION}\\" >/dev/null\\n    elif nvm_rc_version >/dev/null 2>&1; then\\n      nvm install >/dev/null\\n    fi\\n  elif [ \\"_$NVM_MODE\\" = \'_use\' ]; then\\n    NVM_CURRENT=\\"$(nvm_ls_current)\\"\\n    echo NVM_CURRENT=\\"${NVM_CURRENT}\\"\\n    if [ \\"_${NVM_CURRENT}\\" = \'_none\' ] || [ \\"_${NVM_CURRENT}\\" = \'_system\' ]; then\\n      export TIMEFORMAT=\'%R nvm_resolve_local_alias\'\\n      time VERSION=\\"$(nvm_resolve_local_alias default 2>/dev/null || nvm_echo)\\"\\n      if [ -n \\"${VERSION}\\" ]; then\\n        export TIMEFORMAT=\'%R nvm use\'\\n        time nvm use --silent \\"${VERSION}\\" >/dev/null\\n      elif nvm_rc_version >/dev/null 2>&1; then\\n        nvm use --silent >/dev/null\\n      fi\\n    else\\n      nvm use --silent \\"${NVM_CURRENT}\\" >/dev/null\\n    fi\\n  elif [ \\"_${NVM_MODE}\\" != \'_none\' ]; then\\n    nvm_err \'Invalid auto mode supplied.\'\\n    return 1\\n  fi\\n}\\n```\\n\\nIn some places, I printed out the current time instead of timing the execution:\\n\\n```shell\\n    \\"use\\")\\n      local PROVIDED_VERSION\\n      local NVM_SILENT\\n      local NVM_SILENT_ARG\\n      local NVM_DELETE_PREFIX\\n      NVM_DELETE_PREFIX=0\\n      local NVM_LTS\\n\\n      date +\'1 %s.%N\'\\n      while [ $# -ne 0 ]; do\\n        case \\"$1\\" in\\n          --silent)\\n            NVM_SILENT=1\\n            NVM_SILENT_ARG=\'--silent\'\\n          ;;\\n          --delete-prefix) NVM_DELETE_PREFIX=1 ;;\\n          --) ;;\\n          --lts) NVM_LTS=\'*\' ;;\\n          --lts=*) NVM_LTS=\\"${1##--lts=}\\" ;;\\n          --*) ;;\\n          *)\\n            if [ -n \\"${1-}\\" ]; then\\n              PROVIDED_VERSION=\\"$1\\"\\n            fi\\n          ;;\\n        esac\\n        shift\\n      done\\n\\n      date +\'2 %s.%N\'\\n      if [ -n \\"${NVM_LTS-}\\" ]; then\\n        VERSION=\\"$(nvm_match_version \\"lts/${NVM_LTS:-*}\\")\\"\\n      elif [ -z \\"${PROVIDED_VERSION-}\\" ]; then\\n        NVM_SILENT=\\"${NVM_SILENT:-0}\\" nvm_rc_version\\n        if [ -n \\"${NVM_RC_VERSION-}\\" ]; then\\n          PROVIDED_VERSION=\\"${NVM_RC_VERSION}\\"\\n          VERSION=\\"$(nvm_version \\"${PROVIDED_VERSION}\\")\\"\\n        fi\\n        unset NVM_RC_VERSION\\n        if [ -z \\"${VERSION}\\" ]; then\\n          nvm_err \'Please see `nvm --help` or https://github.com/nvm-sh/nvm#nvmrc for more information.\'\\n          return 127\\n        fi\\n      else\\n        VERSION=\\"$(nvm_match_version \\"${PROVIDED_VERSION}\\")\\"\\n      fi\\n\\n      if [ -z \\"${VERSION}\\" ]; then\\n        >&2 nvm --help\\n        return 127\\n      fi\\n```\\n\\nHowever, now the output seems a bit strange:\\n\\n```txt\\n0.053 /home/william/.bashrc.d/00-default.bashrc\\n0.000 /home/william/.bashrc.d/50-android-sdk.bashrc\\n0.000 /home/william/.bashrc.d/50-android-studio.bashrc\\n0.003 /home/william/.bashrc.d/50-asdf.bashrc\\nnvm_auto use\\nNVM_CURRENT=none\\n0.126 nvm_resolve_local_alias\\n0.013 nvm_resolve_alias\\n0.009 nvm_resolve_alias\\n0.158 nvm use\\n0.287 nvm use\\n0.324 nvm use\\n0.041 /home/william/.bashrc.d/50-sdkman.bashrc\\n0.000 /home/william/.bashrc.d/90-gradle-opts.bashrc\\n0.028 bashrc check\\n```\\n\\n- it seemed like `50-nvm.bashrc` was never executed at all -- but this is obviously false, because it was executed\\n- it seemed like there were multiple invocations of `nvm use`\\n- the `date` command didn\'t seem to be executed -- or is it?\\n\\nFor the missing `date` command output, it\'s just a matter of shell redirection. Removing the stdout redirection to `/dev/null` shows the output:\\n\\n```shell\\nnvm_auto() {\\n  echo nvm_auto \\"$@\\"\\n  local NVM_MODE\\n  NVM_MODE=\\"${1-}\\"\\n  local VERSION\\n  local NVM_CURRENT\\n  if [ \\"_${NVM_MODE}\\" = \'_install\' ]; then\\n    VERSION=\\"$(nvm_alias default 2>/dev/null || nvm_echo)\\"\\n    if [ -n \\"${VERSION}\\" ]; then\\n      nvm install \\"${VERSION}\\" >/dev/null\\n    elif nvm_rc_version >/dev/null 2>&1; then\\n      nvm install >/dev/null\\n    fi\\n  elif [ \\"_$NVM_MODE\\" = \'_use\' ]; then\\n    NVM_CURRENT=\\"$(nvm_ls_current)\\"\\n    echo NVM_CURRENT=\\"${NVM_CURRENT}\\"\\n    if [ \\"_${NVM_CURRENT}\\" = \'_none\' ] || [ \\"_${NVM_CURRENT}\\" = \'_system\' ]; then\\n      export TIMEFORMAT=\'%R nvm_resolve_local_alias\'\\n      time VERSION=\\"$(nvm_resolve_local_alias default 2>/dev/null || nvm_echo)\\"\\n      if [ -n \\"${VERSION}\\" ]; then\\n        export TIMEFORMAT=\'%R nvm use\'\\n        time nvm use --silent \\"${VERSION}\\"\\n      elif nvm_rc_version >/dev/null 2>&1; then\\n        nvm use --silent >/dev/null\\n      fi\\n    else\\n      nvm use --silent \\"${NVM_CURRENT}\\" >/dev/null\\n    fi\\n  elif [ \\"_${NVM_MODE}\\" != \'_none\' ]; then\\n    nvm_err \'Invalid auto mode supplied.\'\\n    return 1\\n  fi\\n}\\n```\\n\\nThe stderr redirection is not a problem, it can stay there. The `time` command output is a bit more involved, though.\\n\\nAfter reading through the source code, I realized that it was because the environment variable `TIMEFORMAT` kept getting overwritten by our `export` calls. This means that all of these actions are executed inside the current shell context, so every time we do an `export TIMEFORMAT=...`, we are actually overwriting that environment variable. Actually, if we see that all the functions are defined as `function_name() { ... }`, we know that the commands inside the function is executed in the same shell context as the caller.\\n\\nMeanwhile, the variable `TIMEFORMAT` is evaluated when the command executed by `time` returns or exits, so if it was modified during the execution of the command, then the time printed will follow whatever `TIMEFORMAT` contains at that time.\\n\\nOn the other hand, we actually need all of this to be executed in the same shell context, so that the functions can define environment variables that we can use later. Sidenote: this is the only valid function definition in POSIX shell.\\n\\nSo basically what we need to do is, save the environment variable `TIMEFORMAT` before timing the command execution.\\n\\n## A naive \\"fix\\"\\n\\nSo I attempted a naive fix:\\n\\n```shell\\nnvm_process_parameters() {\\n  local NVM_AUTO_MODE\\n  NVM_AUTO_MODE=\'use\'\\n  while [ $# -ne 0 ]; do\\n    case \\"$1\\" in\\n      --install) NVM_AUTO_MODE=\'install\' ;;\\n      --no-use) NVM_AUTO_MODE=\'none\' ;;\\n    esac\\n    shift\\n  done\\n  _TIMEFORMAT=\\"${TIMEFORMAT}\\"\\n  export TIMEFORMAT=\'%R nvm_auto\'\\n  time nvm_auto \\"${NVM_AUTO_MODE}\\"\\n  export TIMEFORMAT=\\"${_TIMEFORMAT}\\"\\n}\\n\\nnvm_process_parameters \\"$@\\"\\n```\\n\\n```shell\\nnvm_auto() {\\n  echo nvm_auto \\"$@\\"\\n  local NVM_MODE\\n  NVM_MODE=\\"${1-}\\"\\n  local VERSION\\n  local NVM_CURRENT\\n  if [ \\"_${NVM_MODE}\\" = \'_install\' ]; then\\n    VERSION=\\"$(nvm_alias default 2>/dev/null || nvm_echo)\\"\\n    if [ -n \\"${VERSION}\\" ]; then\\n      nvm install \\"${VERSION}\\" >/dev/null\\n    elif nvm_rc_version >/dev/null 2>&1; then\\n      nvm install >/dev/null\\n    fi\\n  elif [ \\"_$NVM_MODE\\" = \'_use\' ]; then\\n    NVM_CURRENT=\\"$(nvm_ls_current)\\"\\n    echo NVM_CURRENT=\\"${NVM_CURRENT}\\"\\n    if [ \\"_${NVM_CURRENT}\\" = \'_none\' ] || [ \\"_${NVM_CURRENT}\\" = \'_system\' ]; then\\n      _TIMEFORMAT=\\"${TIMEFORMAT}\\"\\n      export TIMEFORMAT=\'%R nvm_resolve_local_alias\'\\n      time VERSION=\\"$(nvm_resolve_local_alias default 2>/dev/null || nvm_echo)\\"\\n      export TIMEFORMAT=\\"${_TIMEFORMAT}\\"\\n      if [ -n \\"${VERSION}\\" ]; then\\n        _TIMEFORMAT=\\"${TIMEFORMAT}\\"\\n        export TIMEFORMAT=\'%R nvm use\'\\n        time nvm use --silent \\"${VERSION}\\"\\n        export TIMEFORMAT=\\"${_TIMEFORMAT}\\"\\n      elif nvm_rc_version >/dev/null 2>&1; then\\n        nvm use --silent >/dev/null\\n      fi\\n    else\\n      nvm use --silent \\"${NVM_CURRENT}\\" >/dev/null\\n    fi\\n  elif [ \\"_${NVM_MODE}\\" != \'_none\' ]; then\\n    nvm_err \'Invalid auto mode supplied.\'\\n    return 1\\n  fi\\n}\\n```\\n\\nNow we see that `date` is doing its job, but the output from the `time` statements still seem wrong:\\n\\n```txt\\n0.057 /home/william/.bashrc.d/00-default.bashrc\\n0.000 /home/william/.bashrc.d/50-android-sdk.bashrc\\n0.000 /home/william/.bashrc.d/50-android-studio.bashrc\\n0.003 /home/william/.bashrc.d/50-asdf.bashrc\\nnvm_auto use\\nNVM_CURRENT=none\\n0.131 nvm_resolve_local_alias\\n0 1625994490.165539976\\n1 1625994490.172686988\\n2 1625994490.174463233\\n0.012 nvm_resolve_alias\\n3 1625994490.213854097\\n0.010 nvm_resolve_alias\\n4 1625994490.268562513\\n5 1625994490.301861349\\n6 1625994490.304278525\\n7 1625994490.314978462\\n0.152 nvm use\\n0.286 nvm_auto\\n0.313 nvm_auto\\n0.040 /home/william/.bashrc.d/50-sdkman.bashrc\\n0.000 /home/william/.bashrc.d/90-gradle-opts.bashrc\\n0.020 bashrc check\\n```\\n\\n## The proper fix\\n\\nEventually I realized that we need to do a little bit more. When we save the contents of `TIMEFORMAT` in `_TIMEFORMAT`, we are actually overwriting whatever is in `_TIMEFORMAT` previously. What we actually need to do is, we need to use a different environment variable for each level of function invocation.\\n\\nSo this is the first level, which is OK:\\n\\n```shell\\nnvm_process_parameters() {\\n  local NVM_AUTO_MODE\\n  NVM_AUTO_MODE=\'use\'\\n  while [ $# -ne 0 ]; do\\n    case \\"$1\\" in\\n      --install) NVM_AUTO_MODE=\'install\' ;;\\n      --no-use) NVM_AUTO_MODE=\'none\' ;;\\n    esac\\n    shift\\n  done\\n  _TIMEFORMAT=\\"${TIMEFORMAT}\\"\\n  export TIMEFORMAT=\'%R nvm_auto\'\\n  time nvm_auto \\"${NVM_AUTO_MODE}\\"\\n  export TIMEFORMAT=\\"${_TIMEFORMAT}\\"\\n}\\n\\nnvm_process_parameters \\"$@\\"\\n```\\n\\nIt calls `nvm_auto`, which is the second level:\\n\\n```shell\\nnvm_auto() {\\n  echo nvm_auto \\"$@\\"\\n  local NVM_MODE\\n  NVM_MODE=\\"${1-}\\"\\n  local VERSION\\n  local NVM_CURRENT\\n  if [ \\"_${NVM_MODE}\\" = \'_install\' ]; then\\n    VERSION=\\"$(nvm_alias default 2>/dev/null || nvm_echo)\\"\\n    if [ -n \\"${VERSION}\\" ]; then\\n      nvm install \\"${VERSION}\\" >/dev/null\\n    elif nvm_rc_version >/dev/null 2>&1; then\\n      nvm install >/dev/null\\n    fi\\n  elif [ \\"_$NVM_MODE\\" = \'_use\' ]; then\\n    NVM_CURRENT=\\"$(nvm_ls_current)\\"\\n    echo NVM_CURRENT=\\"${NVM_CURRENT}\\"\\n    if [ \\"_${NVM_CURRENT}\\" = \'_none\' ] || [ \\"_${NVM_CURRENT}\\" = \'_system\' ]; then\\n      __TIMEFORMAT=\\"${TIMEFORMAT}\\"\\n      export TIMEFORMAT=\'%R nvm_resolve_local_alias\'\\n      time VERSION=\\"$(nvm_resolve_local_alias default 2>/dev/null || nvm_echo)\\"\\n      export TIMEFORMAT=\\"${__TIMEFORMAT}\\"\\n      if [ -n \\"${VERSION}\\" ]; then\\n        __TIMEFORMAT=\\"${TIMEFORMAT}\\"\\n        export TIMEFORMAT=\'%R nvm use\'\\n        time nvm use --silent \\"${VERSION}\\"\\n        export TIMEFORMAT=\\"${__TIMEFORMAT}\\"\\n      elif nvm_rc_version >/dev/null 2>&1; then\\n        nvm use --silent >/dev/null\\n      fi\\n    else\\n      nvm use --silent \\"${NVM_CURRENT}\\" >/dev/null\\n    fi\\n  elif [ \\"_${NVM_MODE}\\" != \'_none\' ]; then\\n    nvm_err \'Invalid auto mode supplied.\'\\n    return 1\\n  fi\\n}\\n```\\n\\nNote that within the same function, we don\'t need to use different variables to save `TIMEFORMAT`. This is because the `time` invocations are not nested. However, in the above example, we need to use yet another variable in `nvm_resolve_local_alias`:\\n\\n```shell\\nnvm_resolve_local_alias() {\\n  if [ -z \\"${1-}\\" ]; then\\n    return 1\\n  fi\\n\\n  local VERSION\\n  local EXIT_CODE\\n  ___TIMEFORMAT=\\"${TIMEFORMAT}\\"\\n  export TIMEFORMAT=\'%R nvm_resolve_alias\'\\n  time VERSION=\\"$(nvm_resolve_alias \\"${1-}\\")\\"\\n  EXIT_CODE=$?\\n  export TIMEFORMAT=\\"${___TIMEFORMAT}\\"\\n  if [ -z \\"${VERSION}\\" ]; then\\n    return $EXIT_CODE\\n  fi\\n  if [ \\"_${VERSION}\\" != \'_\u221e\' ]; then\\n    nvm_version \\"${VERSION}\\"\\n  else\\n    nvm_echo \\"${VERSION}\\"\\n  fi\\n}\\n```\\n\\n(note that we need to save the return value before restoring the `TIMEFORMAT` variable -- otherwise it will also be overwritten)\\n\\nNow, the output looks correct, and we know the execution time of the separate function calls:\\n\\n```txt\\n0.053 /home/william/.bashrc.d/00-default.bashrc\\n0.000 /home/william/.bashrc.d/50-android-sdk.bashrc\\n0.000 /home/william/.bashrc.d/50-android-studio.bashrc\\n0.002 /home/william/.bashrc.d/50-asdf.bashrc\\nnvm_auto use\\nNVM_CURRENT=none\\n0.108 nvm_resolve_local_alias\\n0 1625995040.289644320\\n1 1625995040.293696818\\n2 1625995040.294921503\\n0.007 nvm_resolve_alias\\n3 1625995040.336312682\\n0.011 nvm_resolve_alias\\n4 1625995040.385616982\\n5 1625995040.417216708\\n6 1625995040.418956680\\n7 1625995040.429000260\\n0.142 nvm use\\n0.253 nvm_auto\\n0.278 /home/william/.bashrc.d/50-nvm.bashrc\\n0.039 /home/william/.bashrc.d/50-sdkman.bashrc\\n0.000 /home/william/.bashrc.d/90-gradle-opts.bashrc\\n0.018 bashrc check\\n```\\n\\nNote that we could not use arrays, because POSIX shell does not have an array type.\\n\\n## An alternative method\\n\\nThere is an alternative method, which is simpler and shows the execution depth, but it is slower:\\n\\n```shell\\nnvm_process_parameters() {\\n  local NVM_AUTO_MODE\\n  NVM_AUTO_MODE=\'use\'\\n  while [ $# -ne 0 ]; do\\n    case \\"$1\\" in\\n      --install) NVM_AUTO_MODE=\'install\' ;;\\n      --no-use) NVM_AUTO_MODE=\'none\' ;;\\n    esac\\n    shift\\n  done\\n  export TIMEFORMAT=\\"${TIMEFORMAT} / nvm_auto\\"\\n  time nvm_auto \\"${NVM_AUTO_MODE}\\"\\n  export TIMEFORMAT=\\"${TIMEFORMAT% / nvm_auto}\\"\\n}\\n\\nnvm_process_parameters \\"$@\\"\\n```\\n\\n```shell\\nnvm_auto() {\\n  echo nvm_auto \\"$@\\"\\n  local NVM_MODE\\n  NVM_MODE=\\"${1-}\\"\\n  local VERSION\\n  local NVM_CURRENT\\n  if [ \\"_${NVM_MODE}\\" = \'_install\' ]; then\\n    VERSION=\\"$(nvm_alias default 2>/dev/null || nvm_echo)\\"\\n    if [ -n \\"${VERSION}\\" ]; then\\n      nvm install \\"${VERSION}\\" >/dev/null\\n    elif nvm_rc_version >/dev/null 2>&1; then\\n      nvm install >/dev/null\\n    fi\\n  elif [ \\"_$NVM_MODE\\" = \'_use\' ]; then\\n    NVM_CURRENT=\\"$(nvm_ls_current)\\"\\n    echo NVM_CURRENT=\\"${NVM_CURRENT}\\"\\n    if [ \\"_${NVM_CURRENT}\\" = \'_none\' ] || [ \\"_${NVM_CURRENT}\\" = \'_system\' ]; then\\n      export TIMEFORMAT=\\"${TIMEFORMAT} / nvm_resolve_local_alias\\"\\n      time VERSION=\\"$(nvm_resolve_local_alias default 2>/dev/null || nvm_echo)\\"\\n      export TIMEFORMAT=\\"${TIMEFORMAT% / nvm_resolve_local_alias}\\"\\n      if [ -n \\"${VERSION}\\" ]; then\\n        export TIMEFORMAT=\\"${TIMEFORMAT} / nvm use\\"\\n        time nvm use --silent \\"${VERSION}\\"\\n        export TIMEFORMAT=\\"${TIMEFORMAT% / nvm use}\\"\\n      elif nvm_rc_version >/dev/null 2>&1; then\\n        nvm use --silent >/dev/null\\n      fi\\n    else\\n      nvm use --silent \\"${NVM_CURRENT}\\" >/dev/null\\n    fi\\n  elif [ \\"_${NVM_MODE}\\" != \'_none\' ]; then\\n    nvm_err \'Invalid auto mode supplied.\'\\n    return 1\\n  fi\\n}\\n```\\n\\n```shell\\nnvm_resolve_local_alias() {\\n  if [ -z \\"${1-}\\" ]; then\\n    return 1\\n  fi\\n\\n  local VERSION\\n  local EXIT_CODE\\n  export TIMEFORMAT=\\"${TIMEFORMAT} / nvm_resolve_alias\\"\\n  time VERSION=\\"$(nvm_resolve_alias \\"${1-}\\")\\"\\n  EXIT_CODE=$?\\n  export TIMEFORMAT=\\"${TIMEFORMAT% / nvm_resolve_alias}\\"\\n  if [ -z \\"${VERSION}\\" ]; then\\n    return $EXIT_CODE\\n  fi\\n  if [ \\"_${VERSION}\\" != \'_\u221e\' ]; then\\n    nvm_version \\"${VERSION}\\"\\n  else\\n    nvm_echo \\"${VERSION}\\"\\n  fi\\n}\\n```\\n\\nWith this method, we get:\\n\\n```\\n0.058 /home/william/.bashrc.d/00-default.bashrc\\n0.000 /home/william/.bashrc.d/50-android-sdk.bashrc\\n0.000 /home/william/.bashrc.d/50-android-studio.bashrc\\n0.005 /home/william/.bashrc.d/50-asdf.bashrc\\nnvm_auto use\\nNVM_CURRENT=none\\n0.109 /home/william/.bashrc.d/50-nvm.bashrc / nvm_auto / nvm_resolve_local_alias\\n0 1625995835.146498759\\n1 1625995835.151210563\\n2 1625995835.152251905\\n0.009 /home/william/.bashrc.d/50-nvm.bashrc / nvm_auto / nvm use / nvm_resolve_alias\\n3 1625995835.195350192\\n0.011 /home/william/.bashrc.d/50-nvm.bashrc / nvm_auto / nvm use / nvm_resolve_alias\\n4 1625995835.248658001\\n5 1625995835.280006594\\n6 1625995835.282036172\\n7 1625995835.291081025\\n0.148 /home/william/.bashrc.d/50-nvm.bashrc / nvm_auto / nvm use\\n0.262 /home/william/.bashrc.d/50-nvm.bashrc / nvm_auto\\n0.289 /home/william/.bashrc.d/50-nvm.bashrc\\n0.040 /home/william/.bashrc.d/50-sdkman.bashrc\\n0.000 /home/william/.bashrc.d/90-gradle-opts.bashrc\\n0.025 bashrc check\\n```\\n\\nThese two methods should be enough for now. I do wonder if we can use `local` variables for this, and if we can actually avoid the `export`s -- well, that\'s another blog post for another day, and maybe I\'ll make a custom shell script to demonstrate that.\\n\\n## Conclusion\\n\\nAll in all, today we have learned:\\n\\n1. there is almost always more than one way to do it\\n1. shell is not that simple, we need to think of execution context before overwriting variables\\n1. check to see if stdout is redirected, otherwise some output may be missing\\n1. be careful when adding profiling into code, make sure that return values are saved"},{"id":"/2021/07/10/bashrc-directory","metadata":{"permalink":"/blog/2021/07/10/bashrc-directory","editUrl":"https://github.com/wpyoga/wpyoga.github.io/edit/master/blog/2021-07-10-bashrc-directory.md","source":"@site/blog/2021-07-10-bashrc-directory.md","title":"Using a `.bashrc.d` directory instead of just `.bashrc`","description":"When we install a new development environment, the installer will usually modify /.bashrc or /.profile. However, this is not very clean, because sometimes the entries are not removed when the development environment is uninstalled.","date":"2021-07-10T00:00:00.000Z","formattedDate":"July 10, 2021","tags":[{"label":"bash","permalink":"/blog/tags/bash"},{"label":"bashrc","permalink":"/blog/tags/bashrc"},{"label":"bashrc.d","permalink":"/blog/tags/bashrc-d"},{"label":"shell","permalink":"/blog/tags/shell"}],"truncated":true,"authors":[],"prevItem":{"title":"Manual Profiling of Bash Script Execution","permalink":"/blog/2021/07/11/manual-profiling-bash-script"},"nextItem":{"title":"GitHub `main` branch","permalink":"/blog/2021/06/16/github-main-branch"}},"content":"When we install a new development environment, the installer will usually modify `~/.bashrc` or `~/.profile`. However, this is not very clean, because sometimes the entries are not removed when the development environment is uninstalled.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Why use a directory?\\n\\nBy using a directory, we can add and remove customizations by simply adding and removing files. Consider this example: when we install [SDKMAN!](https://sdkman.io/), the installer adds these lines to `~/.bashrc` by default:\\n\\n```shell\\n#THIS MUST BE AT THE END OF THE FILE FOR SDKMAN TO WORK!!!\\nexport SDKMAN_DIR=\\"/home/william/.sdkman\\"\\n[[ -s \\"/home/william/.sdkman/bin/sdkman-init.sh\\" ]] && source \\"/home/william/.sdkman/bin/sdkman-init.sh\\"\\n```\\n\\nThen when we uninstall SDKMAN!, we have to open the `~/.bashrc` file and remove the lines that were added.\\n\\nThe same thing happens with `nvm`, a popular version manager for `Node.js`.\\n\\nSome installers even had to let the user edit `~/.bashrc` by hand.\\n\\n## Historical precedence\\n\\nActually, this is not a new idea. The directory `/etc/profile.d` exists for the same reason, albeit for software installed directly onto the system. The contents of this directory is sourced in by `/etc/profile`, as seen in the snippet below:\\n\\n```shell\\nif [ -d /etc/profile.d ]; then\\n  for i in /etc/profile.d/*.sh; do\\n    if [ -r $i ]; then\\n      . $i\\n    fi\\n  done\\n  unset i\\nfi\\n```\\n\\nAlso, someone else has [blogged about this topic](https://waxzce.medium.com/use-bashrc-d-directory-instead-of-bloated-bashrc-50204d5389ff) on Medium. However, some Medium articles are blocked by a paywall, and I\'m not sure if that article is a paid article, or will be made a paid article at some point in the future, so I\'m making this blog post instead. In contrast, this blog will always be free, at least as long as GitHub continues to provide GitHub Pages for free.\\n\\n## Further information\\n\\n`~/.bashrc.d`\\n\\n- (2002) https://bugs.gentoo.org/4854\\n- (2011) http://blogs.perl.org/users/chisel/2011/08/managing-my-shell-setup.html\\n- (2012) https://groups.google.com/g/linux.debian.bugs.dist/c/1mDbDViPFFQ\\n- (2012) https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=675008\\n- (2015) https://freesoft.dev/program/31617374\\n- (2015) https://blog.sanctum.geek.nz/tag/bashrc-d/\\n- (2016) https://lsbbugs.linuxfoundation.org/show_bug.cgi?id=4167\\n- (2016) https://github.com/jdcapa/bashrc.d\\n- (2017) https://waxzce.medium.com/use-bashrc-d-directory-instead-of-bloated-bashrc-50204d5389ff\\n- (2017) https://github.com/oskar404/.bashrc.d\\n- (2019) https://sneak.berlin/20191011/stupid-unix-tricks/\\n- (2019) https://bugzilla.redhat.com/show_bug.cgi?id=1726397\\n- (2019) https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=931353\\n- (2019) https://bugs.launchpad.net/ubuntu/+source/bash/+bug/1835077\\n- (2020) https://source.arknet.ch/fmorgner/dotfiles/-/tree/16f950b2ce140a81e45bcad99ea142a6f3f7a7f2/bashrc.d\\n- (2020) https://dev.to/swiknaba/how-to-organize-your-bash-profile-20eb\\n- (2020) https://lab.retarded.farm/zappel/zGentoo-playground/-/tree/22e288c8b0ed166d1bc0050f9f00e3fe4708b0d4/etc/bash/bashrc.d\\n- (2021) https://blog.jbriault.fr/bashrc-d/\\n- https://g.gg42.eu/framagit/conf-99-basic_config_debian/commit/56f82be3cc0f98af05989f34f44817e90d0e814f\\n- https://write.as/bpsylevc6lliaspe\\n- https://timnash.co.uk/bashing-my-bashrc-productivity-fridays/\\n\\n\\n`/etc/profile.d`:\\n\\n- https://eng.libretexts.org/Bookshelves/Computer_Science/Operating_Systems/Linux_-_The_Penguin_Marches_On_(McClanahan)/02%3A_User_Group_Administration/5.03%3A_System_Wide_User_Profiles/5.03.2_System_Wide_User_Profiles%3A_The_etc-profile.d_Directory\\n- (2014) https://askubuntu.com/questions/438150/why-are-scripts-in-etc-profile-d-being-ignored-system-wide-bash-aliases\\n\\n\\n`profile` and `bashrc` in general:\\n\\n- (2013) https://bencane.com/2013/09/16/understanding-a-little-more-about-etcprofile-and-etcbashrc/\\n- (2017) https://scriptingosx.com/2017/04/about-bash_profile-and-bashrc-on-macos/\\n\\n## Simple implementation\\n\\nTo implement the `~/.bashrc.d` scheme, we first need to make that directory, and then move the original rc file into that directory:\\n\\n```shell\\n$ cd\\n$ mkdir .bashrc.d\\n$ mv -i .bashrc .bashrc.d/00-default.bashrc\\n```\\n\\nAfter that, we need to make a replacement `~/.bashrc` file that sources all the scripts inside `~/.bashrc.d`. This is basically an adaptation of the code snippet in `/etc/profile`:\\n\\n```shell\\nfor i in \\"${HOME}/.bashrc.d\\"/[0-9][0-9]-*.bashrc; do\\n  if [ -r \\"$i\\" ]; then\\n    . \\"$i\\"\\n  fi\\ndone\\nunset i\\n```\\n\\nNote that we don\'t need to test if the directory is present -- if it\'s not present, then `i` will look something like `/home/william/.bashrc.d/*.bashrc`, so the following test for readability will fail anyway.\\n\\nWith this scheme, when you install a new software or SDK into your personal home directory, just add a new file named something like `50-file.bashrc` in `~/.bashrc.d` and it will be sourced in during shell startup.\\n\\n## Self-cleaning implementation\\n\\nWith the above implementation, some scripts will still directly modify `~/.bashrc`, and ignore the `~/.bashrc.d` directory. We can actually make a self-cleaning implementation, so that lines appended to `~/.bashrc` will automatically be moved into `~/.bashrc.d`.\\n\\nAn easy way to do that, is by using markers in `~/.bashrc`. The markers tell us when the original file starts and ends, so that we can move all the other content into `~/.bashrc.d`.\\n\\nTo achieve this, we modify `~/.bashrc` even further:\\n\\n```shell\\n### BASHRC START\\nfor i in \\"${HOME}/.bashrc.d\\"/[0-9][0-9]-*.bashrc; do\\n  if [ -r \\"$i\\" ]; then\\n    . \\"$i\\"\\n  fi\\ndone\\nunset i\\n\\nif which mktemp >/dev/null 2>&1; then\\n  BASHRC_BEFORE=\\"$(mktemp \\"${HOME}/.bashrc.d/50-new-XXXXXX\\")\\"\\n  BASHRC_AFTER=\\"$(mktemp \\"${HOME}/.bashrc.d/50-new-XXXXXX\\")\\"\\n  BASHRC_TEMP=\\"$(mktemp \\"${HOME}/.bashrc-XXXXXX\\")\\"\\n  BASHRC_MODE=before\\n  while IFS= read -r LINE; do\\n    if [ \\"${LINE}\\" = \'### BASHRC START\' ]; then\\n      BASHRC_MODE=bashrc\\n    elif [ \\"${LINE}\\" = \'### BASHRC END\' ]; then\\n      BASHRC_MODE=after\\n      continue\\n    fi\\n\\n    if [ \\"${BASHRC_MODE}\\" = \'before\' ]; then\\n      printf \'%s\\\\n\' \\"${LINE}\\" >>\\"${BASHRC_BEFORE}\\"\\n    elif [ \\"${BASHRC_MODE}\\" = \'after\' ]; then\\n      printf \'%s\\\\n\' \\"${LINE}\\" >>\\"${BASHRC_AFTER}\\"\\n    else\\n      printf \'%s\\\\n\' \\"${LINE}\\" >>\\"${BASHRC_TEMP}\\"\\n    fi\\n  done <\\"${HOME}/.bashrc\\"\\n  # this marker is never printed in the loop\\n  echo \'### BASHRC END\' >>\\"${BASHRC_TEMP}\\"\\n  mv \\"${BASHRC_TEMP}\\" \\"${HOME}/.bashrc\\"\\n\\n  if [ -s \\"${BASHRC_BEFORE}\\" -o -s \\"${BASHRC_AFTER}\\" ]; then\\n    echo \\"~/.bashrc has been modified. Check the following files for customizations:\\"\\n    [ -s \\"${BASHRC_BEFORE}\\" ] && echo \\"* ${BASHRC_BEFORE}\\"\\n    [ -s \\"${BASHRC_AFTER}\\" ] && echo \\"* ${BASHRC_AFTER}\\"\\n  else\\n    [ ! -s \\"${BASHRC_BEFORE}\\" ] && rm \\"${BASHRC_BEFORE}\\"\\n    [ ! -s \\"${BASHRC_AFTER}\\" ] && rm \\"${BASHRC_AFTER}\\"\\n  fi\\n  unset BASHRC_BEFORE BASHRC_AFTER BASHRC_TEMP BASHRC_MODE\\nfi\\n### BASHRC END\\n```\\n\\nThis looks bloated and unnecessary. But this section takes only 9 ms on my laptop (i5-5200u), so it may not be so slow after all. Still, if you think it\'s too complicated, you can use the [simple implementation](#simple-implementation) instead."},{"id":"/2021/06/16/github-main-branch","metadata":{"permalink":"/blog/2021/06/16/github-main-branch","editUrl":"https://github.com/wpyoga/wpyoga.github.io/edit/master/blog/2021-06-16-github-main-branch.md","source":"@site/blog/2021-06-16-github-main-branch.md","title":"GitHub `main` branch","description":"I just found out today that GitHub has changed the default name of the master branch to \\"main\\". This is supposedly done to avoid any references to slavery (master, slave, get it?), but I think it\'s a load of bullshit.","date":"2021-06-16T00:00:00.000Z","formattedDate":"June 16, 2021","tags":[{"label":"github","permalink":"/blog/tags/github"},{"label":"politics","permalink":"/blog/tags/politics"},{"label":"political-correctness","permalink":"/blog/tags/political-correctness"}],"truncated":true,"authors":[],"prevItem":{"title":"Using a `.bashrc.d` directory instead of just `.bashrc`","permalink":"/blog/2021/07/10/bashrc-directory"},"nextItem":{"title":"Possible dirname bug in Webpack","permalink":"/blog/2021/06/13/possible-webpack-dirname-bug"}},"content":"I just found out today that GitHub has changed the default name of the `master` branch to \\"main\\". This is supposedly done to avoid any references to slavery (master, slave, get it?), but I think it\'s a load of bullshit.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Disclaimer\\n\\nDo I support slavery? Hell no!\\n\\nDo I support [political correctness](https://en.wikipedia.org/wiki/Political_correctness)? Fuck no!\\n\\nDoes naming the default branch `master` instead of something else mean you support slavery? Of course not!\\n\\nAnyway, I have always wondered why when I create a new repo in GitHub, it seems to want me to create a \\"main\\" branch instead of the more familiar `master` branch. Now I know why. And now I know how far behind I am on the current state of affairs.\\n\\nOh, and by the way, not mentioning `master` and `slave` in everyday life does not protect minorities and people born to slavery. It just makes us forget about humanity\'s ugly side, and may even lead some to re-enact slavery in the future. When you don\'t know the pain of being burned in a fire, you don\'t care when someone wants to start a fire. When you don\'t know the ugly truth behind slavery, you don\'t care when someone devises a \\"cool\\" way to re-establish slavery.\\n\\n## Regaining some sanity\\n\\nIf you have accidentally created a \\"main\\" branch, you can safely rename it. First, rename the branch locally:\\n\\n```shell-session\\n$ git branch --move main master\\n```\\n\\nThen, push the `master` branch to GitHub:\\n\\n```shell-session\\n$ git push --set-upstream origin master\\n$ git remote set-head origin master\\n```\\n\\nVoil\xe0! Now you have regained sanity, saved a few brain cells, and can work normally again.\\n\\n## Avoiding hassle of renaming branches\\n\\nGitHub now changes the default branch name to \\"main\\". To avoid the hassle of renaming the `master` branch of future repositories, open the [repository configuration page](https://github.com/settings/repositories) and change the `Repository default branch` setting to `master`, then click `Update`. New repositories will have a `master` branch by default.\\n\\n## Closing rant\\n\\nThere is a nice comment on [this article](https://www.zdnet.com/article/github-to-replace-master-with-main-starting-next-month/):\\n\\n> Now I\'m gobsmacked (can I say \\"gobsmacked\\" or does that offend victims of physical violence?). This is ridiculous, where do you draw the line!?\\n> Has this terminology even been tested with the people who are expected to be offended by it!?\\n> i\'m all for equality and fully support the BLM movement - ALL people should be treated with the same respect - but actions such as this do nothing to help the movement, they just serve to further divide.\\n> History is exactly that, it happened. Ceasing to use a word does not erase the action that it may once have been associated with. You can\'t scrub away all references to poor treatment, if you try, there will be no safe words left.\\n\\nSee also the replies in this [email thread](https://lore.kernel.org/git/CAOAHyQwyXC1Z3v7BZAC+Bq6JBaM7FvBenA-1fcqeDV==apdWDg@mail.gmail.com/t/).\\n\\nI will not be bullied into changing my repository default branch names to \\"main\\". Neither will I deceive myself by thinking that using \\"main\\" instead of `master` is better. When `git` changes its default branch to \\"main\\", I will follow suit. I hope that day does not come too soon, but with [Linus apologizing for \\"being a jerk\\"](https://arstechnica.com/gadgets/2018/09/linus-torvalds-apologizes-for-years-of-being-a-jerk-takes-time-off-to-learn-empathy/), I wonder how soon that day will come.\\n\\nMay humankind survive the incoming onslaught of snowflakes."},{"id":"/2021/06/13/possible-webpack-dirname-bug","metadata":{"permalink":"/blog/2021/06/13/possible-webpack-dirname-bug","editUrl":"https://github.com/wpyoga/wpyoga.github.io/edit/master/blog/2021-06-13-possible-webpack-dirname-bug.md","source":"@site/blog/2021-06-13-possible-webpack-dirname-bug.md","title":"Possible dirname bug in Webpack","description":"I got an error while trying to build this blog in a temporary directory. This might be a webpack bug -- or not.","date":"2021-06-13T00:00:00.000Z","formattedDate":"June 13, 2021","tags":[{"label":"docusaurus","permalink":"/blog/tags/docusaurus"},{"label":"blog","permalink":"/blog/tags/blog"}],"truncated":true,"authors":[],"prevItem":{"title":"GitHub `main` branch","permalink":"/blog/2021/06/16/github-main-branch"},"nextItem":{"title":"Deploying Docusaurus site to GitHub the easy way","permalink":"/blog/2021/06/12/docusaurus-deploy"}},"content":"I got an error while trying to build this blog in a temporary directory. This might be a webpack bug -- or not.\\n\\n\x3c!-- truncate --\x3e\\n\\n```shell-session\\n$ yarn build --out-dir \\"${TMPDIR}\\"\\nyarn run v1.22.10\\n$ docusaurus build --out-dir /tmp/tmp.pEBNPFYg2h\\n\\n[en] Creating an optimized production build...\\n\\n\u2714 Client\\n  \\n\\n\u25cf Server \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 cache (99%) shutdown IdleFileCachePlugin\\n stored\\n\\nError: EISDIR: illegal operation on a directory, open \'/tmp/tmp.pEBNPFYg2h\'\\nerror building locale=en\\nError: EISDIR: illegal operation on a directory, open \'/tmp/tmp.pEBNPFYg2h\'\\nerror Command failed with exit code 1.\\ninfo Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.\\n```\\n\\nSo I started investigating -- first figuring out where the problem is.\\nI thought it was due to permissions, but I changed the directory permissions to `0775` and it still didn\'t work.\\nI thought it was due to the temporary directory being outside of the source directory, so I tried using `mktemp -d -p .` to make the temporary in the current directory instead -- but it still failed. This is when I realized that the trigger is the dot character in the directory name.\\n\\nI found the script executing the `build` command at `node_modules/@docusaurus/core/lib/commands/build.js`. I added a few checkpoints, and re-ran the script:\\n\\n```shell-session\\n$ yarn build --out-dir test.dir\\nyarn run v1.22.10\\n$ docusaurus build --out-dir test.dir\\n\\n[en] Creating an optimized production build...\\ncheck 100\\ncheck 200\\ncheck 300\\n\\n\u2714 Client\\n\\n\u25cf Client \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 cache (99%)  \\n\\n\u2714 Client\\n  \\n\\n\u25cf Server \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 cache (99%) shutdown IdleFileCachePlugin\\n stored\\n\\nError: EISDIR: illegal operation on a directory, open \'/home/william/Documents/wpyoga/wpyoga.github.io/test.dir\'\\nerror building locale=en\\nError: EISDIR: illegal operation on a directory, open \'/home/william/Documents/wpyoga/wpyoga.github.io/test.dir\'\\nerror Command failed with exit code 1.\\ninfo Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.\\n```\\n\\nThis is where it failed:\\n\\n```js\\nconsole.log(\\"check 300\\");\\n    // Run webpack to build JS bundle (client) and static html files (server).\\n    await utils_1.compile([clientConfig, serverConfig]);\\nconsole.log(\\"check 350\\");\\n```\\n\\nIt turned out, `utils_1` is a webpack wrapper (??? maybe, not sure, I don\'t know the pattern):\\n\\n```js\\nconst utils_1 = require(\\"../webpack/utils\\");\\n```\\n\\nIn the file `node_modules/@docusaurus/core/lib/webpack/utils.js`, there is a function `compile()`:\\n\\n```js\\nfunction compile(config) {\\n    return new Promise((resolve, reject) => {\\n        const compiler = webpack_1.default(config);\\n        compiler.run((err, stats) => {\\n```\\n\\nAnd `webpack_1` is:\\n\\n```js\\nconst webpack_1 = tslib_1.__importDefault(require(\\"webpack\\"));\\n```\\n\\nIt looks like a bug (or feature, keep reading) in webpack. I haven\'t tried to debug this yet.\\n\\n## Possibly a feature\\n\\nSome googling led me to this page: https://stackoverflow.com/questions/37070141/webpack-dev-server-allow-paths-with-dot-in-them\\n\\nIt seems that webpack, or one of its plugins, try to differentiate files from directories by checking whether it has a dot in its name. So when it sees a dot, it assumes that the entity is a file instead of a directory.\\n\\nThese commands work though, because the file name does not contain a dot. Only the pathname does, and the preceding path components cannot be files.\\n\\n```shell-session\\n$ yarn build --out-dir tmp.123/aaa\\n$ yarn build --out-dir /tmp/tpm.123456/bbb\\n```"},{"id":"/2021/06/12/docusaurus-deploy","metadata":{"permalink":"/blog/2021/06/12/docusaurus-deploy","editUrl":"https://github.com/wpyoga/wpyoga.github.io/edit/master/blog/2021-06-12-docusaurus-deploy.md","source":"@site/blog/2021-06-12-docusaurus-deploy.md","title":"Deploying Docusaurus site to GitHub the easy way","description":"Before, we need to run this command to deploy a Docusaurus site to GitHub Pages:","date":"2021-06-12T00:00:00.000Z","formattedDate":"June 12, 2021","tags":[{"label":"docusaurus","permalink":"/blog/tags/docusaurus"},{"label":"blog","permalink":"/blog/tags/blog"}],"truncated":true,"authors":[],"prevItem":{"title":"Possible dirname bug in Webpack","permalink":"/blog/2021/06/13/possible-webpack-dirname-bug"},"nextItem":{"title":"Making a new blog with Docusaurus v2","permalink":"/blog/2021/06/12/docusaurus-install"}},"content":"Before, we need to run this command to deploy a Docusaurus site to GitHub Pages:\\n\\n```shell-session\\n$ GIT_USER=wpyoga DEPLOYMENT_BRANCH=gh-pages USE_SSH=true yarn deploy\\n```\\n\\nIf we don\'t use SSH, we will be prompted for a password. Right now I don\'t even know my own password (it\'s stored in my password manager), so this is not a good idea. And I already use SSH to work on GitHub repositories anyway. So this is not a good way to do things.\\n\\n\x3c!-- truncate --\x3e\\n\\nBut wait -- if we already use SSH, why do we need to specify GIT_USER again? That\'s exactly... what another user mentioned in [Docusaurus#3454](https://github.com/facebook/docusaurus/issues/3454).\\n\\nLet\'s take this a few steps further:\\n\\n- If we use SSH, our remote origin URL looks like this: `git@github.com:wpyoga/wpyoga.github.io.git`\\n    ```shell-session\\n    $ git config --get remote.origin.url\\n    git@github.com:wpyoga/wpyoga.github.io.git\\n    ```\\n    So why do we need to specify `USE_SSH=true`?\\n\\n- And if we use SSH, when pushing to the upstream branch, we don\'t even need to specify the username -- git takes care of this for us. So why do we need to specify `GIT_USER`?\\n\\n- As for the deployment branch: yes, the default is `master` for the special repository `_username_.github.io`, but I don\'t like this pattern, and I want to use `gh-pages` for my `wpyoga.github.io` repo. This configuration is usually static, so let\'s store it inside `docusaurus.config.js`:\\n    ```js\\n    module.exports = {\\n      // ...\\n      projectName: \'wpyoga.github.io\',\\n      deploymentBranch: \'gh-pages\',\\n      // ...\\n    ```\\n\\nNow, `docusaurus deploy` doesn\'t understand these new mechanisms. And since I don\'t have a working TypeScript development environment (I\'m not there yet...) I can only edit the generated JavaScript by hand. The functionality is in the file `node_modules/@docusaurus/core/lib/commands/deploy.js`.\\n\\nSo I made a backup at `node_modules/@docusaurus/core/lib/commands/deploy.js.orig` and made a few changes:\\n\\n```diff\\n--- node_modules/@docusaurus/core/lib/commands/deploy.js.orig\\t2021-06-12 11:49:51.340710466 +0700\\n+++ node_modules/@docusaurus/core/lib/commands/deploy.js\\t2021-06-12 12:51:51.064280040 +0700\\n@@ -42,8 +42,16 @@\\n     if (!shelljs_1.default.which(\'git\')) {\\n         throw new Error(\'Git not installed or on the PATH!\');\\n     }\\n+    var _useSSH = process.env.USE_SSH || \'false\'; // make sure useSSH is always defined\\n+    if (_useSSH !== \'true\') {\\n+        const remoteOriginUrl = shelljs_1.default.exec(\'git config --get remote.origin.url\').stdout.trim();\\n+        if (remoteOriginUrl.match(/^ssh:\\\\/\\\\//) !== null || remoteOriginUrl.match(/^([\\\\w\\\\-]+@)?[\\\\w.\\\\-]+:[\\\\w.\\\\-\\\\/_]+\\\\.git/) !== null) {\\n+            _useSSH = \'true\';\\n+        }\\n+    }\\n+    const useSSH = _useSSH === \'true\' ? \'true\' : \'false\';\\n     const gitUser = process.env.GIT_USER;\\n-    if (!gitUser) {\\n+    if (!gitUser && useSSH.toLocaleLowerCase() !== \'true\') {\\n         throw new Error(\'Please set the GIT_USER environment variable!\');\\n     }\\n     // The branch that contains the latest docs changes that will be deployed.\\n@@ -71,6 +79,7 @@\\n     }\\n     // github.io indicates organization repos that deploy via master. All others use gh-pages.\\n     const deploymentBranch = process.env.DEPLOYMENT_BRANCH ||\\n+        siteConfig.deploymentBranch ||\\n         (projectName.indexOf(\'.github.io\') !== -1 ? \'master\' : \'gh-pages\');\\n     console.log(`${chalk_1.default.cyan(\'deploymentBranch:\')} ${deploymentBranch}`);\\n     const githubHost = process.env.GITHUB_HOST || siteConfig.githubHost || \'github.com\';\\n@@ -80,7 +89,6 @@\\n     if (gitPass) {\\n         gitCredentials = `${gitCredentials}:${gitPass}`;\\n     }\\n-    const useSSH = process.env.USE_SSH;\\n     const remoteBranch = buildRemoteBranchUrl_1.buildUrl(githubHost, githubPort, gitCredentials, organizationName, projectName, useSSH !== undefined && useSSH.toLowerCase() === \'true\');\\n     console.log(`${chalk_1.default.cyan(\'Remote branch:\')} ${obfuscateGitPass(remoteBranch)}`);\\n     // Check if this is a cross-repo publish.\\n```\\n\\nBut then `yarn build` complains:\\n\\n```shell-session\\n$ yarn build\\nyarn run v1.22.10\\n$ docusaurus build\\n\\nA validation error occured.\\nThe validation system was added recently to Docusaurus as an attempt to avoid user configuration errors.\\nWe may have made some mistakes.\\nIf you think your configuration is valid and should keep working, please open a bug report.\\n\\nError: These field(s) [\\"deploymentBranch\\",] are not recognized in docusaurus.config.js.\\nIf you still want these fields to be in your configuration, put them in the \'customFields\' attribute.\\nSee https://docusaurus.io/docs/docusaurus.config.js/#customfields\\n    at Object.validateConfig (/home/william/Documents/wpyoga/wpyoga.github.io/node_modules/@docusaurus/core/lib/server/configValidation.js:118:15)\\n    at Object.loadConfig [as default] (/home/william/Documents/wpyoga/wpyoga.github.io/node_modules/@docusaurus/core/lib/server/config.js:18:31)\\n    at Object.loadContext (/home/william/Documents/wpyoga/wpyoga.github.io/node_modules/@docusaurus/core/lib/server/index.js:38:47)\\n    at build (/home/william/Documents/wpyoga/wpyoga.github.io/node_modules/@docusaurus/core/lib/commands/build.js:45:36)\\n    at /home/william/Documents/wpyoga/wpyoga.github.io/node_modules/@docusaurus/core/bin/docusaurus.js:94:5\\n    at Command.<anonymous> (/home/william/Documents/wpyoga/wpyoga.github.io/node_modules/@docusaurus/core/bin/docusaurus.js:126:23)\\n    at Command.listener [as _actionHandler] (/home/william/Documents/wpyoga/wpyoga.github.io/node_modules/@docusaurus/core/node_modules/commander/index.js:413:31)\\n    at Command._parseCommand (/home/william/Documents/wpyoga/wpyoga.github.io/node_modules/@docusaurus/core/node_modules/commander/index.js:914:14)\\n    at Command._dispatchSubcommand (/home/william/Documents/wpyoga/wpyoga.github.io/node_modules/@docusaurus/core/node_modules/commander/index.js:865:18)\\n    at Command._parseCommand (/home/william/Documents/wpyoga/wpyoga.github.io/node_modules/@docusaurus/core/node_modules/commander/index.js:882:12)\\n    at Command.parse (/home/william/Documents/wpyoga/wpyoga.github.io/node_modules/@docusaurus/core/node_modules/commander/index.js:717:10)\\n    at run (/home/william/Documents/wpyoga/wpyoga.github.io/node_modules/@docusaurus/core/bin/docusaurus.js:319:7)\\n    at Object.<anonymous> (/home/william/Documents/wpyoga/wpyoga.github.io/node_modules/@docusaurus/core/bin/docusaurus.js:326:1)\\n    at Module._compile (internal/modules/cjs/loader.js:999:30)\\n    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1027:10)\\n    at Module.load (internal/modules/cjs/loader.js:863:32)\\nerror Command failed with exit code 1.\\ninfo Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.\\n```\\n\\nSo we patch another file:\\n\\n```diff\\n--- ./node_modules/@docusaurus/core/lib/server/configValidation.js.orig\\t2021-06-12 12:41:58.223529946 +0700\\n+++ ./node_modules/@docusaurus/core/lib/server/configValidation.js\\t2021-06-12 12:42:10.131464512 +0700\\n@@ -69,6 +69,7 @@\\n         .default(exports.DEFAULT_CONFIG.onDuplicateRoutes),\\n     organizationName: utils_validation_1.Joi.string().allow(\'\'),\\n     projectName: utils_validation_1.Joi.string().allow(\'\'),\\n+    deploymentBranch: utils_validation_1.Joi.string().allow(\'\'),\\n     customFields: utils_validation_1.Joi.object().unknown().default(exports.DEFAULT_CONFIG.customFields),\\n     githubHost: utils_validation_1.Joi.string(),\\n     plugins: utils_validation_1.Joi.array().items(PluginSchema).default(exports.DEFAULT_CONFIG.plugins),\\n```\\n\\nAnd now `yarn build` succeeds:\\n\\n```shell-session\\n$ yarn build\\nyarn run v1.22.10\\n$ docusaurus build\\n\\n[en] Creating an optimized production build...\\n\\n\u2714 Client\\n  \\n\\n\u2714 Server\\n  Compiled successfully in 9.46s\\n\\n\\n\u2714 Client\\n  \\n\\n\u25cf Server \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 cache (99%) shutdown IdleFileCachePlugin\\n stored\\n\\nSuccess! Generated static files in build.\\n\\nUse `npm run serve` to test your build locally.\\n\\nDone in 12.32s.\\n```\\n\\nAnd `yarn deploy` also succeeds:\\n\\n```shell-session\\n$ yarn deploy\\nyarn run v1.22.10\\n$ docusaurus deploy\\nDeploy command invoked ...\\ngit@github.com:wpyoga/wpyoga.github.io.git\\nmaster\\norganizationName: wpyoga\\nprojectName: wpyoga.github.io\\ndeploymentBranch: gh-pages\\nRemote branch: git@github.com:wpyoga/wpyoga.github.io.git\\ngit@github.com:wpyoga/wpyoga.github.io.git\\n77dc4151c656e3239bd9742f912a45db18bcb462\\nCMD: git rev-parse HEAD (code=0)\\n\\n[en] Creating an optimized production build...\\n\\n\u2714 Client\\n  \\n\\n\u2714 Server\\n  Compiled successfully in 9.22s\\n\\n\\n\u2714 Client\\n  \\n\\n\u25cf Server \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 cache (99%) shutdown IdleFileCachePlugin\\n stored\\n\\nSuccess! Generated static files in build.\\n\\nUse `npm run serve` to test your build locally.\\n\\nCloning into \'/tmp/wpyoga.github.io-gh-pagesSj6zQx\'...\\nCMD: git clone git@github.com:wpyoga/wpyoga.github.io.git /tmp/wpyoga.github.io-gh-pagesSj6zQx (code=0)\\nmaster\\nNote: switching to \'origin/gh-pages\'.\\n\\nYou are in \'detached HEAD\' state. You can look around, make experimental\\nchanges and commit them, and you can discard any commits you make in this\\nstate without impacting any branches by switching back to a branch.\\n\\nIf you want to create a new branch to retain commits you create, you may\\ndo so (now or later) by using -c with the switch command. Example:\\n\\n  git switch -c <new-branch-name>\\n\\nOr undo this operation with:\\n\\n  git switch -\\n\\nTurn off this advice by setting config variable advice.detachedHead to false\\n\\nHEAD is now at ab5aa5e Deploy website - based on 77dc4151c656e3239bd9742f912a45db18bcb462\\nCMD: git checkout origin/gh-pages (code=0)\\nSwitched to a new branch \'gh-pages\'\\nCMD: git checkout -b gh-pages (code=0)\\nBranch \'gh-pages\' set up to track remote branch \'gh-pages\' from \'origin\'.\\nCMD: git branch --set-upstream-to=origin/gh-pages (code=0)\\nrm \'.nojekyll\'\\nrm \'404.html\'\\nrm \'assets/css/styles.dc5e9681.css\'\\nrm \'assets/images/docsVersionDropdown-dda80f009a926fb2dd92bab8faa6c4d8.png\'\\nrm \'assets/images/localeDropdown-0052c3f08ccaf802ac733b23e655f498.png\'\\nrm \'assets/js/01a85c17.b9dfffa7.js\'\\nrm \'assets/js/0ae750d7.e3503f7e.js\'\\nrm \'assets/js/0e384e19.ddf5b862.js\'\\nrm \'assets/js/17896441.f57a37b4.js\'\\nrm \'assets/js/18c41134.ec87b176.js\'\\nrm \'assets/js/1be78505.2dfa4c90.js\'\\nrm \'assets/js/1e4232ab.5b4de121.js\'\\nrm \'assets/js/1f391b9e.2de59b39.js\'\\nrm \'assets/js/393be207.8e037661.js\'\\nrm \'assets/js/41215780.ae841bb1.js\'\\nrm \'assets/js/486.57d4378a.js\'\\nrm \'assets/js/486.57d4378a.js.LICENSE.txt\'\\nrm \'assets/js/4bddfbdb.01575b48.js\'\\nrm \'assets/js/50f28d20.45bef03f.js\'\\nrm \'assets/js/533a09ca.48ff7b50.js\'\\nrm \'assets/js/5c868d36.0d5626a9.js\'\\nrm \'assets/js/608.4d5a0579.js\'\\nrm \'assets/js/611.cf04f8c7.js\'\\nrm \'assets/js/631037e5.846f9692.js\'\\nrm \'assets/js/6422631d.05d2e755.js\'\\nrm \'assets/js/6875c492.c165e8d1.js\'\\nrm \'assets/js/796.2b4d523f.js\'\\nrm \'assets/js/822bd8ab.16dcaaf8.js\'\\nrm \'assets/js/935f2afb.721a68e8.js\'\\nrm \'assets/js/9abc614d.1dedaf98.js\'\\nrm \'assets/js/9dfd250b.026c9da7.js\'\\nrm \'assets/js/a6aa9e1f.3d8aec90.js\'\\nrm \'assets/js/a7023ddc.492f117e.js\'\\nrm \'assets/js/a80da1cf.9fe763a5.js\'\\nrm \'assets/js/b2b675dd.ebac9b0c.js\'\\nrm \'assets/js/b85f0a39.51abaa96.js\'\\nrm \'assets/js/bcafcfd6.24ddec43.js\'\\nrm \'assets/js/c4f5d8e4.aa71182e.js\'\\nrm \'assets/js/c623835b.36f855cd.js\'\\nrm \'assets/js/ccc49370.08e8cf99.js\'\\nrm \'assets/js/dff1c289.ae41d539.js\'\\nrm \'assets/js/e44a2883.2df5a1b2.js\'\\nrm \'assets/js/f55d3e7a.012746c9.js\'\\nrm \'assets/js/fd7cafb0.62cfb1d0.js\'\\nrm \'assets/js/main.2654cb24.js\'\\nrm \'assets/js/main.2654cb24.js.LICENSE.txt\'\\nrm \'assets/js/runtime~main.6059adcf.js\'\\nrm \'blog/2021/05/08/inotifywait-problem/index.html\'\\nrm \'blog/2021/06/12/docusaurus-install/index.html\'\\nrm \'blog/atom.xml\'\\nrm \'blog/index.html\'\\nrm \'blog/rss.xml\'\\nrm \'blog/tags/blog/index.html\'\\nrm \'blog/tags/docusaurus/index.html\'\\nrm \'blog/tags/index.html\'\\nrm \'blog/tags/inotifywait/index.html\'\\nrm \'blog/tags/maven/index.html\'\\nrm \'blog/tags/vscode/index.html\'\\nrm \'docs/hello/index.html\'\\nrm \'docs/intro/index.html\'\\nrm \'docs/tutorial-basics/congratulations/index.html\'\\nrm \'docs/tutorial-basics/create-a-blog-post/index.html\'\\nrm \'docs/tutorial-basics/create-a-document/index.html\'\\nrm \'docs/tutorial-basics/create-a-page/index.html\'\\nrm \'docs/tutorial-basics/deploy-your-site/index.html\'\\nrm \'docs/tutorial-basics/markdown-features/index.html\'\\nrm \'docs/tutorial-extras/manage-docs-versions/index.html\'\\nrm \'docs/tutorial-extras/translate-your-site/index.html\'\\nrm \'img/docusaurus.png\'\\nrm \'img/favicon.ico\'\\nrm \'img/logo.svg\'\\nrm \'img/tutorial/docsVersionDropdown.png\'\\nrm \'img/tutorial/localeDropdown.png\'\\nrm \'img/undraw_docusaurus_mountain.svg\'\\nrm \'img/undraw_docusaurus_react.svg\'\\nrm \'img/undraw_docusaurus_tree.svg\'\\nrm \'index.html\'\\nrm \'markdown-page/index.html\'\\nrm \'new-markdown-page/index.html\'\\nrm \'new-react-page/index.html\'\\nrm \'sitemap.xml\'\\nCMD: git rm -rf . (code=0)\\nCMD: git add --all (code=0)\\n[gh-pages 1d4c705] Deploy website - based on 77dc4151c656e3239bd9742f912a45db18bcb462\\n 46 files changed, 209 insertions(+), 131 deletions(-)\\n create mode 100644 assets/js/0ae750d7.36dc1408.js\\n delete mode 100644 assets/js/0ae750d7.e3503f7e.js\\n rename assets/js/{41215780.ae841bb1.js => 41215780.9cfec5e3.js} (82%)\\n rename assets/js/{631037e5.846f9692.js => 631037e5.a502de85.js} (75%)\\n rename assets/js/{a7023ddc.492f117e.js => a7023ddc.ca586d5f.js} (77%)\\n rename assets/js/{a80da1cf.9fe763a5.js => a80da1cf.6a3d81ae.js} (70%)\\n rename assets/js/{b2b675dd.ebac9b0c.js => b2b675dd.87aa17ab.js} (60%)\\n rename assets/js/{b85f0a39.51abaa96.js => b85f0a39.40fba475.js} (50%)\\n rename assets/js/{bcafcfd6.24ddec43.js => bcafcfd6.b73d2137.js} (63%)\\n create mode 100644 assets/js/e620ed35.d4befd8b.js\\n create mode 100644 assets/js/fba230db.cc24edfc.js\\n delete mode 100644 assets/js/fd7cafb0.62cfb1d0.js\\n create mode 100644 assets/js/fd7cafb0.6d8981c1.js\\n delete mode 100644 assets/js/main.2654cb24.js\\n create mode 100644 assets/js/main.72a8bb2f.js\\n rename assets/js/{main.2654cb24.js.LICENSE.txt => main.72a8bb2f.js.LICENSE.txt} (100%)\\n delete mode 100644 assets/js/runtime~main.6059adcf.js\\n create mode 100644 assets/js/runtime~main.e809c4a5.js\\n create mode 100644 blog/2021/06/12/docusaurus-deploy/index.html\\n rewrite blog/2021/06/12/docusaurus-install/index.html (76%)\\n rewrite sitemap.xml (74%)\\nCMD: git commit -m \\"Deploy website - based on 77dc4151c656e3239bd9742f912a45db18bcb462\\" (code=0)\\nTo github.com:wpyoga/wpyoga.github.io.git\\n   ab5aa5e..1d4c705  gh-pages -> gh-pages\\nCMD: git push --force origin gh-pages (code=0)\\nWebsite is live at https://wpyoga.github.io/\\nDone in 22.54s.\\n```\\n\\nSomeday I will submit a proper patch to Docusaurus. Might also be a good way to start learning TypeScript (and JavaScript)!"},{"id":"/2021/06/12/docusaurus-install","metadata":{"permalink":"/blog/2021/06/12/docusaurus-install","editUrl":"https://github.com/wpyoga/wpyoga.github.io/edit/master/blog/2021-06-12-docusaurus-install.md","source":"@site/blog/2021-06-12-docusaurus-install.md","title":"Making a new blog with Docusaurus v2","description":"To make a new blog with the upcoming Docusaurus 2, we need understand how GitHub Pages works, and also pay attention to several caveats.","date":"2021-06-12T00:00:00.000Z","formattedDate":"June 12, 2021","tags":[{"label":"docusaurus","permalink":"/blog/tags/docusaurus"},{"label":"blog","permalink":"/blog/tags/blog"}],"truncated":true,"authors":[],"prevItem":{"title":"Deploying Docusaurus site to GitHub the easy way","permalink":"/blog/2021/06/12/docusaurus-deploy"},"nextItem":{"title":"Problem with inotifywait, VSCode, and Maven","permalink":"/blog/2021/05/08/inotifywait-problem"}},"content":"To make a new blog with the upcoming Docusaurus 2, we need understand how GitHub Pages works, and also pay attention to several caveats.\\n\\n\x3c!-- truncate --\x3e\\n\\n## How GitHub Pages works and how to use it\\n\\nGitHub Pages sites serve its root as static content from a directory on a selected branch of the special repository `_username_.github.io` or `_organization_.github.io`. If there is an `index.html` file, then it will be used. Otherwise, `README.md` will be served -- but see [this note](#how-github-pages-really-works).\\n\\nFor example, I have the username `wpyoga` on GitHub. My GitHub Pages site is `wpyoga.github.io`, and it serves its root `https://wpyoga.github.io/` as static content from the `master` branch of the repository `wpyoga.github.io` in my account.\\n\\nAs of 2021-06-12, the special repository `_username_.github.io` defaults to serving GitHub Pages from the `master` branch, and other repositories default to serving from the `gh-pages` branch. This means that:\\n\\n- on repositories other than the special repository, if you push to the `gh-pages` branch, and no branch has been configured to serve the GitHub Pages site, then GitHub will automatically designate the `gh-pages` branch for GitHub Pages, and serve the site from the repository\'s root directory.\\n\\n- on the special repository, pushing to the `master` branch will trigger the same mechanism. Pushing to the `gh-pages` branch will not trigger this mechanism.\\n\\nNote that this designated branch contains the generated static content, not the raw Markdown and the sources. So if you use Docusaurus (or Jekyll) to generate the static site from Markdown documents, then the designated branch should not contain the sources, but only the generated (built) content. Docusaurus follows GitHub Pages\' convention, so by default it will try to publish the generated content in the `build` directory of the source tree to the designated branch, according to the repository name.\\n\\nHowever, my preferred method is to use the `master` branch for the source tree, and push my generated content to the `gh-pages` branch. Note that for the special repository, the `gh-pages` branch doesn\'t trigger the automatic mechanism above, so I had to [manually configure it](https://github.com/wpyoga/wpyoga.github.io/settings/branches). Alternatively, you can also [follow GitHub conventions](#following-github-conventions).\\n\\n### Deploy\\n\\n`yarn deploy` will build and deploy the site:\\n\\n```shell-session\\n$ GIT_USER=wpyoga DEPLOYMENT_BRANCH=gh-pages USE_SSH=true yarn deploy\\n```\\n\\nIt doesn\'t look pretty, and it\'s not easy to remember. Fortunately, I have [a better solution](/blog/2021/06/12/docusaurus-deploy).\\n\\n## Further discussion\\n\\n### Manually deploying to a branch\\n\\nFirst, checkout the site serving branch (`gh-pages` in our example) to a temporary directory:\\n\\n```shell-session\\n$ git worktree add --no-checkout /tmp/build gh-pages\\n```\\n\\nGenerate the site and copy it into the temporary directory:\\n\\n```shell-session\\n$ yarn build\\n$ cp -rT build /tmp/build\\n```\\n\\nNotes:\\n- we cannot use `yarn build --out-dir /tmp/build` because this will remove the `.git` file inside that directory\\n- with the `-T` option, `cp` copies the contents `build` directory into `/tmp/build`, as opposed to copying the `build` directory itself\\n- there is a better way if you want to use a script, see the end of this section\\n\\nPush the files to GitHub:\\n\\n```shell-session\\n$ cd /tmp/build\\n$ git add .\\n$ git commit -m \\"new build at $(date)\\"\\n$ git push\\n$ cd -\\n```\\n\\nRemove the temporary directory:\\n\\n```shell-session\\n$ git worktree remove /tmp/build\\n```\\n\\nHere\'s a script to automate the whole process:\\n\\n```shell\\n#!/bin/sh\\n\\nTMPDIR=\\"$(mktemp -d tmp-XXXXX)\\"\\nTMPDIR2=\\"$(mktemp -d tmp-XXXXX)\\"\\n\\ngit worktree add --force --no-checkout \\"${TMPDIR}\\" gh-pages\\n\\nmv \\"${TMPDIR}/.git\\" \\"${TMPDIR2}\\"\\nyarn build --out-dir \\"${TMPDIR}\\"\\nmv \\"${TMPDIR2}/.git\\" \\"${TMPDIR}\\"\\nrmdir \\"${TMPDIR2}\\"\\n\\n(cd \\"${TMPDIR}\\"; git add .; git commit -m \\"new build at $(date)\\"; git push)\\n\\nls \\"${TMPDIR}\\"\\ngit worktree remove \\"${TMPDIR}\\"\\n```\\n\\nNote: we use a custom template because there [might be a bug](/blog/2021/06/13/possible-webpack-dirname-bug) that causes `yarn build` to fail.\\n\\n### Another method of deploying manually doesn\'t work well\\n\\nAfter building the site, push the `build` directory to a new `gh-pages` branch:\\n\\n```shell-session\\n$ git subtree split -P build -b gh-pages\\nCreated branch \'gh-pages\'\\nd7d2eaa20128724d8234c817151c16d0931dec98\\n$ git push origin gh-pages\\nEnumerating objects: 117, done.\\nCounting objects: 100% (117/117), done.\\nDelta compression using up to 4 threads\\nCompressing objects: 100% (83/83), done.\\nWriting objects: 100% (117/117), 322.93 KiB | 1.02 MiB/s, done.\\nTotal 117 (delta 29), reused 54 (delta 8)\\nremote: Resolving deltas: 100% (29/29), done.\\nremote: \\nremote: Create a pull request for \'gh-pages\' on GitHub by visiting:\\nremote:      https://github.com/wpyoga/wpyoga.github.io/pull/new/gh-pages\\nremote: \\nTo github.com:wpyoga/wpyoga.github.io.git\\n * [new branch]      gh-pages -> gh-pages\\n```\\n\\nBecause this repository is the special `_username_.github.io` repository, GitHub doesn\'t treat the `gh-pages` branch here as being special. So I need to [manually specify](https://github.com/wpyoga/wpyoga.github.io/settings/pages) the `gh-pages` branch to serve the site root.\\n\\nThe problem is, now I cannot update the `gh-pages` branch. I need to read Docusaurus\' source code in `lib/deploy.ts` and see how they manage to make `yarn deploy` work. Looking at the log output, it seems that Docusaurus clones the `gh-pages` branch to a temporary location, deletes everything, overwrites the content with new files, and then pushes the changes back to the remote branch.\\n\\n### How GitHub Pages really works\\n\\nI\'m not 100% sure how it works.\\n\\nDocusaurus mentions that [GitHub will run the generated files through Jekyll](https://docusaurus.io/docs/deployment#deploying-to-github-pages), so a `.nojekyll` file is added to each directory, to prevent the removal of files whose names have a leading `_` (underscore).\\n\\nI\'ve also tried messing around with the designated repository, renaming index.html and adding README.md, but the Docusaurus site is still being served (a \\"page not found\\" message can be seen upon initial loading, but then disappears after a split second).\\n\\nSince this is a static site, index.php is not served (at all).\\n\\n### Serving GitHub Pages from a subdirectory\\n\\nWe can actually use a single branch for both the source code and generated content -- GitHub allows us to specify the `/docs` subdirectory as the content location. However, this is not easily adaptable to Docusaurus, where `docs` stores Markdown files, and the generated content is actually in `build`.\\n\\nUnfortunately, as of 2021-06-12, GitHub doesn\'t allow the use of any other subdirectory for this purpose.\\n\\n### Following GitHub conventions\\n\\nFor the special repository, following GitHub\'s conventions is somewhat counterintuitive. You would usually have the source code in the `master` branch, and in this case you cannot deploy to the `master` branch since it will overwrite all the source code. In this case, the solution is to move the source code to another branch, say `source`, work on that branch, and deploy to the `master` branch.\\n\\nI pushed the source code from the `master` branch to the `source` branch.\\n\\n```shell-session\\n$ git branch source\\n$ git push --set-upstream origin source\\n```\\n\\nThen I would need to delete the `master` branch. The problem is, GitHub didn\'t want to delete the `master` branch:\\n\\n```shell-session\\n$ git push --delete origin master\\nTo github.com:wpyoga/wpyoga.github.io.git\\n ! [remote rejected] master (refusing to delete the current branch: refs/heads/master)\\nerror: failed to push some refs to \'git@github.com:wpyoga/wpyoga.github.io.git\'\\n```\\n\\nIt turns out that GitHub just doesn\'t want to delete the default branch. Note that this concept of \\"default branch\\" is not from git, but rather from GitHub.\\n\\nAnyway, I changed the [default branch](https://github.com/wpyoga/wpyoga.github.io/settings/branches) to `source` for now, and now I can delete the `master` branch:\\n\\n```shell-session\\n$ git push --delete origin master\\nTo github.com:wpyoga/wpyoga.github.io.git\\n - [deleted]         master\\n```\\n\\nNow, `yarn deploy` will push the generated content to the `master` branch of the special repository.\\n\\n### Moving source code back to the master branch\\n\\nAfter I moved the source code from the `master` branch to the `source` branch, I found out that I can actually deploy to another branch instead.\\nSo I wanted to move the source code back to the `master` branch.\\n\\nI deleted the `master` branch, renamed the `source` branch to `master`, and then pushed the changes:\\n\\n```shell-session\\n$ git branch -D master\\nDeleted branch master (was 67c2b46).\\n$ git branch -m source master\\n$ git status\\nOn branch master\\nYour branch is up to date with \'origin/source\'.\\n$ git push origin HEAD\\nTotal 0 (delta 0), reused 0 (delta 0)\\nremote: \\nremote: Create a pull request for \'master\' on GitHub by visiting:\\nremote:      https://github.com/wpyoga/wpyoga.github.io/pull/new/master\\nremote: \\nTo github.com:wpyoga/wpyoga.github.io.git\\n * [new branch]      HEAD -> master\\n```\\n\\nAt this point the `source` branch was useless, so I changed the default branch to `master` again (on GitHub), then deleted the remote `source` branch:\\n\\n```shell-session\\n$ git push --delete origin source\\nTo github.com:wpyoga/wpyoga.github.io.git\\n - [deleted]         source\\n```\\n\\nAt this point the remote branch has been deleted, but the local repo still references the old one:\\n\\n```shell-session\\n$ git status\\nOn branch master\\nYour branch is based on \'origin/source\', but the upstream is gone.\\n  (use \\"git branch --unset-upstream\\" to fixup)\\n```\\n\\nSo I dutifully followed the recommendations:\\n\\n```shell-session\\n$ git branch --unset-upstream\\n$ git push --set-upstream origin master\\nBranch \'master\' set up to track remote branch \'master\' from \'origin\'.\\nEverything up-to-date\\n$ git status\\nOn branch master\\nYour branch is up to date with \'origin/master\'.\\n```\\n\\n### Changing usernames or organization names\\n\\nI used to have the username `wpyh`, but I changed it a few weeks ago.\\n\\nWhen I had the username `wpyh`, if I had created a repository named `wpyh.github.io`, then GitHub would have made a subdomain for me: `wpyh.github.io`. This is a special repo, which serves as the source of the GitHub Pages site hosted at `wpyh.github.io` from its `main` branch by default.\\n\\nWhen I changed my username from `wpyh` to `wpyoga`, I would have had to rename the aforementioned repository to `wpyoga.github.io`. GitHub would have changed the custom subdomain to `wpyoga.github.io`, and the old custom subdomain would have been deleted."},{"id":"/2021/05/08/inotifywait-problem","metadata":{"permalink":"/blog/2021/05/08/inotifywait-problem","editUrl":"https://github.com/wpyoga/wpyoga.github.io/edit/master/blog/2021-05-08-inotifywait-problem.md","source":"@site/blog/2021-05-08-inotifywait-problem.md","title":"Problem with inotifywait, VSCode, and Maven","description":"When doing some Java development on VSCode, I wanted to use inotifywait to watch for source file changes and automatically build the project when a source file is changed.","date":"2021-05-08T00:00:00.000Z","formattedDate":"May 8, 2021","tags":[{"label":"inotifywait","permalink":"/blog/tags/inotifywait"},{"label":"vscode","permalink":"/blog/tags/vscode"},{"label":"maven","permalink":"/blog/tags/maven"}],"truncated":true,"authors":[],"prevItem":{"title":"Making a new blog with Docusaurus v2","permalink":"/blog/2021/06/12/docusaurus-install"}},"content":"When doing some Java development on VSCode, I wanted to use inotifywait to watch for source file changes and automatically build the project when a source file is changed.\\n\\nIt seemed like a neat and simple idea. However, as soon as I set up inotifywait to watch the source directory, I ran into problems and had to abandon the idea (for the time being).\\n\\n\x3c!-- truncate --\x3e\\n\\nSo the project is built using Maven. The problems are:\\n\\n- VSCode cannot resolve source package files: the IDE shows errors on imports from source packages inside the source tree. Imports from the Java library work fine though.\\n\\n- Maven doesn\'t build source package files: the produced jar file doesn\'t contain the class files anymore.\\n\\nIt\'s like the files just went missing -- what did inotifywait do to cause this problem? Reading the manpage, it seems that inotifywait does little more than put inotify(7) watches recursively on the `src` directory."}]}')}}]);